---
slug: k3s-vllm-serving
title: "[Infra] K3sì— vLLMìœ¼ë¡œ LLM ì„œë¹™í•˜ê¸°"
authors: [me]
tags: [k3s, vllm, qwen, llm, serving, gpu, nvidia]
---

ì§€ë‚œ í¬ìŠ¤íŒ…ì—ì„œ **K3s + NVIDIA GPU** í™˜ê²½ì„ êµ¬ì¶•í•˜ê³ , í•„ìˆ˜ ëª…ë ¹ì–´(`kubectl`)ê¹Œì§€ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤.
ì´ì œ ë“œë””ì–´ **ì‹¤ì „**ì…ë‹ˆë‹¤! ì´ë²ˆ í¬ìŠ¤íŒ…ë¶€í„° 2íšŒì— ê±¸ì³, ëŒ€í‘œì ì¸ LLM ì„œë¹™ ë„êµ¬ì¸ **vLLM**ê³¼ **Ollama**ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ëª¨ë¸ì„ ë°°í¬í•´ë³´ê² ìŠµë‹ˆë‹¤.

**1íƒ„: ì••ë„ì ì¸ ì„±ëŠ¥ê³¼ í™•ì¥ì„±, vLLM í¸**
2íƒ„: ê°€ë³ê³  ê°„í¸í•œ ë¡œì»¬ ì‹¤í–‰, Ollama í¸

ì´ë²ˆ ê¸€ì—ì„œëŠ” ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½ì—ì„œ í‘œì¤€ì²˜ëŸ¼ ìë¦¬ ì¡ì€ ê³ ì„±ëŠ¥ ì¶”ë¡  ì—”ì§„ **vLLM**ì„ ì‚¬ìš©í•˜ì—¬, ê°€ë³ê³  ê°•ë ¥í•œ **Qwen3-4B-Thinking** ëª¨ë¸ì„ ì„œë¹™í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë¦…ë‹ˆë‹¤.

<!--truncate-->

## 1. ì™œ vLLMì¸ê°€? (Why vLLM?)

LLMì„ ì„œë¹™í•  ë•Œ ë‹¨ìˆœíˆ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ëª¨ë¸ì„ ë¡œë“œí•´ì„œ ì¶”ë¡ í•˜ë©´ ì†ë„ê°€ ë§¤ìš° ëŠë¦¬ê³ , ë™ì‹œ ì ‘ì†ìê°€ ëŠ˜ì–´ë‚  ë•Œ ë©”ëª¨ë¦¬ ë¶€ì¡±(OOM) í˜„ìƒì´ ë¹ˆë²ˆí•˜ê²Œ ë°œìƒí•©ë‹ˆë‹¤.

**vLLM**ì€ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë“±ì¥í•œ ì´ˆê³ ì† LLM ì¶”ë¡  ì—”ì§„ì…ë‹ˆë‹¤.

### 1.1 ì••ë„ì ì¸ ì²˜ë¦¬ëŸ‰ (Throughput)
vLLMì˜ í•µì‹¬ ê¸°ìˆ ì¸ **PagedAttention** ì•Œê³ ë¦¬ì¦˜ì€ ìš´ì˜ì²´ì œì˜ ê°€ìƒ ë©”ëª¨ë¦¬ ê´€ë¦¬ ê¸°ë²•ì—ì„œ ì˜ê°ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.
ì´ë¥¼ í†µí•´ KV Cache(Key-Value ìºì‹œ) ë©”ëª¨ë¦¬ ë‚­ë¹„ë¥¼ ì¤„ì—¬, ê°™ì€ GPUì—ì„œë„ **ìµœëŒ€ 24ë°° ë” ë§ì€ ìš”ì²­**ì„ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 1.2 ìµœì‹  ëª¨ë¸ì˜ ë°œ ë¹ ë¥¸ ì§€ì›
ìƒˆë¡œìš´ ëª¨ë¸ ì•„í‚¤í…ì²˜(Llama 3, Qwen 2.5/3, Mistral ë“±)ê°€ ì¶œì‹œë˜ë©´, vLLM ì»¤ë®¤ë‹ˆí‹°ëŠ” ê±°ì˜ ì‹¤ì‹œê°„ìœ¼ë¡œ ì´ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
ë”°ë¼ì„œ ìµœì‹  SOTA(State-of-the-Art) ëª¨ë¸ì„ ê°€ì¥ ë¨¼ì €, ê°€ì¥ ë¹ ë¥´ê²Œ ì„œë¹„ìŠ¤ì— ì ìš©í•˜ê³  ì‹¶ë‹¤ë©´ vLLM ì‚¬ìš©ë²•ì„ ìµíˆëŠ” ê²ƒì´ í•„ìˆ˜ì…ë‹ˆë‹¤.

### 1.3 ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì…ê³¼ ì–‘ìí™” ì§€ì›
vLLMì€ ë‹¤ì–‘í•œ ì •ë°€ë„(Precision)ì™€ ì••ì¶• ê¸°ìˆ ì„ ì§€ì›í•˜ì—¬, ë¦¬ì†ŒìŠ¤ ìƒí™©ì— ë§ê²Œ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

*   **FP16 / BF16**: ê¸°ë³¸ ë°˜ì •ë°€ë„. BF16ì€ A100/H100 ë“± Ampere ì´ìƒ ì•„í‚¤í…ì²˜ì—ì„œ ë” ë„“ì€ í‘œí˜„ ë²”ìœ„ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ/ì¶”ë¡ ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
*   **FP8**: ìµœì‹  Hopper ì•„í‚¤í…ì²˜(H100) ë“±ì—ì„œ ì§€ì›í•˜ë©°, ì„±ëŠ¥ ì €í•˜ ì—†ì´ ëª¨ë¸ í¬ê¸°ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ê³  ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤. [^1]
*   **GPTQ / AWQ / INT4**: ê°€ì¤‘ì¹˜(Weight)ë¥¼ 4ë¹„íŠ¸ ì •ìˆ˜ë¡œ ì–‘ìí™”í•˜ê±°ë‚˜, í™œì„±í™”(Activation)ê¹Œì§€ ê³ ë ¤í•˜ì—¬ ì••ì¶•í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. VRAM ìš©ëŸ‰ì´ ì ì€ ì†Œë¹„ììš© GPU(RTX 30/40 ì‹œë¦¬ì¦ˆ)ì—ì„œë„ ê±°ëŒ€ ëª¨ë¸ì„ ëŒë¦´ ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. [^2][^3]
*   **MXFP4**: ìµœì‹  Blackwell ì•„í‚¤í…ì²˜ë¥¼ ìœ„í•œ 4ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  í¬ë§·ìœ¼ë¡œ, ê·¹ë„ë¡œ íš¨ìœ¨ì ì¸ ëŒ€ì—­í­ í™œìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
*   **Multi-LoRA**: í•˜ë‚˜ì˜ ë² ì´ìŠ¤ ëª¨ë¸ì— ì—¬ëŸ¬ ê°œì˜ LoRA ì–´ëŒ‘í„°ë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œí•˜ì—¬, ìš”ì²­ë§ˆë‹¤ ë‹¤ë¥¸ í˜ë¥´ì†Œë‚˜ì˜ ë´‡ì„ ì„œë¹™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

[^1]: [FP8 Formats for Deep Learning](https://arxiv.org/abs/2209.05433)
[^2]: [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)
[^3]: [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978)

---

## 2. vLLM ì£¼ìš” ì˜µì…˜ (Quick Reference)

vLLMì€ ë§¤ìš° ë§ì€ ì‹¤í–‰ ì˜µì…˜ì„ ì œê³µí•©ë‹ˆë‹¤. ì‹¤ë¬´ì—ì„œ ê°€ì¥ ìì£¼ ë§ˆì£¼ì¹˜ê²Œ ë  í•µì‹¬ ì˜µì…˜ 3ê°€ì§€ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

| ì˜µì…˜ | ì„¤ëª… | ê¶Œì¥ ì„¤ì •ê³¼ íŒ |
| :--- | :--- | :--- |
| `--model` | HuggingFace ëª¨ë¸ ID ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ | ì˜ˆ: `Qwen/Qwen3-4B-Thinking` |
| `--gpu-memory-utilization` | GPU ë©”ëª¨ë¦¬ ì¤‘ KV Cacheì— í• ë‹¹í•  ë¹„ìœ¨ | ê¸°ë³¸ê°’ `0.9`. OOMì´ ë°œìƒí•˜ë©´ `0.85`, `0.8` ë“±ìœ¼ë¡œ ì¡°ê¸ˆì”© ë‚®ì¶°ë³´ì„¸ìš”. |
| `--max-model-len` | ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (í† í° ìˆ˜) | ëª¨ë¸ì˜ ìŠ¤í™ì„ ë„˜ê¸¸ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GPU VRAMì´ ë¶€ì¡±í•˜ë©´ ì´ ê°’ì„ ì¤„ì—¬ì„œ OOMì„ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜ˆ: 4096, 8192) |

<details>
<summary>ğŸ”½ <strong>(í´ë¦­) vLLM v0.15.1 Stable ì „ì²´ ì˜µì…˜ ì •ë¦¬</strong></summary>

ì´ ì„¹ì…˜ì€ **vLLM v0.15.1 Stable** ë²„ì „ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„¸í•œ ë‚´ìš©ì€ ë³„ë„ ë¬¸ì„œì¸ **[vLLM ì˜µì…˜ ì™„ë²½ ì •ë¦¬](./vLLM_OPTIONS.md)** ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

### 2.1 Model Arguments (ëª¨ë¸ ê´€ë ¨ ì„¤ì •)

| Argument | Default | Description |
| :--- | :--- | :--- |
| `--model` | (í•„ìˆ˜) | ì‚¬ìš©í•  ëª¨ë¸ì˜ HuggingFace ID ë˜ëŠ” ë¡œì»¬ ê²½ë¡œì…ë‹ˆë‹¤. |
| `--tokenizer` | `None` | ëª¨ë¸ê³¼ ë‹¤ë¥¸ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•  ê²½ìš° ì§€ì •í•©ë‹ˆë‹¤. ë¯¸ì§€ì • ì‹œ `--model`ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •ë©ë‹ˆë‹¤. |
| `--skip-tokenizer-init` | `False` | í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”ë¥¼ ê±´ë„ˆë›¸ì§€ ì—¬ë¶€ì…ë‹ˆë‹¤. ì„ë² ë”© ëª¨ë¸ ë“± íŠ¹ìˆ˜ ëª©ì  ì‹œ ì‚¬ìš©ë©ë‹ˆë‹¤. |
| `--revision` | `None` | HuggingFace ëª¨ë¸ì˜ íŠ¹ì • ë¦¬ë¹„ì „(ë¸Œëœì¹˜, íƒœê·¸)ì„ ê³ ì •í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤. |
| `--trust-remote-code` | `False` | ì›ê²© ì½”ë“œë¥¼ ì‹ ë¢°í•˜ê³  ì‹¤í–‰í• ì§€ ì—¬ë¶€ì…ë‹ˆë‹¤. (ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ëª¨ë¸ ì‚¬ìš© ì‹œ í•„ìˆ˜) |
| `--dtype` | `auto` | ëª¨ë¸ ê°€ì¤‘ì¹˜ ë° ì—°ì‚° ë°ì´í„° íƒ€ì…ì…ë‹ˆë‹¤. Ampere ì´ìƒ GPUëŠ” `bfloat16`ì„ ê¶Œì¥í•©ë‹ˆë‹¤. |
| `--max-model-len` | `None` | ëª¨ë¸ì˜ ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì œí•œí•˜ì—¬ OOMì„ ë°©ì§€í•©ë‹ˆë‹¤. |

### 2.2 Parallel Arguments (ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •)

| Argument | Default | Description |
| :--- | :--- | :--- |
| `--tensor-parallel-size`, `-tp` | `1` | í…ì„œ ë³‘ë ¬ ì²˜ë¦¬(Tensor Parallelism)ì— ì‚¬ìš©í•  GPU ê°œìˆ˜ì…ë‹ˆë‹¤. |
| `--pipeline-parallel-size`, `-pp` | `1` | íŒŒì´í”„ë¼ì¸ ë³‘ë ¬ ì²˜ë¦¬(Pipeline Parallelism)ì— ì‚¬ìš©í•  GPU ê°œìˆ˜ì…ë‹ˆë‹¤. |
| `--gpu-memory-utilization` | `0.9` | vLLM í”„ë¡œì„¸ìŠ¤ê°€ ì‚¬ìš©í•  GPU ë©”ëª¨ë¦¬ ë¹„ìœ¨(0~1)ì…ë‹ˆë‹¤. OOM ë°œìƒ ì‹œ ë‚®ì¶°ì•¼ í•©ë‹ˆë‹¤. |

### 2.3 KV Cache Arguments (ìºì‹œ ë©”ëª¨ë¦¬ ê´€ë¦¬)

| Argument | Default | Description |
| :--- | :--- | :--- |
| `--block-size` | `16` | PagedAttention ë¸”ë¡ í¬ê¸°ì…ë‹ˆë‹¤. |
| `--enable-prefix-caching` | `False` | í”„ë¡¬í”„íŠ¸ ì ‘ë‘ì‚¬(Prefix) ìºì‹±ì„ í™œì„±í™”í•˜ì—¬ ë°˜ë³µ ìš”ì²­ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤. |
| `--swap-space` | `4` | GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ KV ìºì‹œë¥¼ ì˜¤í”„ë¡œë”©í•  CPU ë©”ëª¨ë¦¬ í¬ê¸°(GiB)ì…ë‹ˆë‹¤. |

### 2.4 Scheduler Arguments (ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •)

| Argument | Default | Description |
| :--- | :--- | :--- |
| `--max-num-batched-tokens` | `None` | í•œ ë²ˆì˜ ë°˜ë³µ ë‹¹ ì²˜ë¦¬í•  ìµœëŒ€ í† í° ìˆ˜ì…ë‹ˆë‹¤. |
| `--max-num-seqs` | `256` | í•œ ë²ˆì˜ ë°˜ë³µ ë‹¹ ì²˜ë¦¬í•  ìµœëŒ€ ì‹œí€€ìŠ¤(ìš”ì²­) ê°œìˆ˜ì…ë‹ˆë‹¤. |
| `--chunked-prefill-enabled` | `False` | ê¸´ í”„ë¡¬í”„íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•˜ì—¬ ì§€ì—° ì‹œê°„ì„ ì¤„ì…ë‹ˆë‹¤. |

### 2.5 LoRA & Quantization (ê¸°íƒ€ ì„¤ì •)

| Argument | Default | Description |
| :--- | :--- | :--- |
| `--enable-lora` | `False` | LoRA ì–´ëŒ‘í„° ì‚¬ìš©ì„ í™œì„±í™”í•©ë‹ˆë‹¤. |
| `--max-loras` | `1` | ë™ì‹œì— í™œì„±í™”í•  ìˆ˜ ìˆëŠ” LoRA ì–´ëŒ‘í„°ì˜ ìµœëŒ€ ê°œìˆ˜ì…ë‹ˆë‹¤. |
| `--quantization`, `-q` | `None` | ì–‘ìí™” ë°©ì‹ì„ ì§€ì •í•©ë‹ˆë‹¤. (`awq`, `gptq`, `fp8` ë“±) |
| `--kv-cache-dtype` | `auto` | KV ìºì‹œ ì €ì¥ ë°ì´í„° íƒ€ì…ì…ë‹ˆë‹¤. `fp8` ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. |

> **ì°¸ê³ **: vLLMì€ ì—…ë°ì´íŠ¸ ì†ë„ê°€ ë§¤ìš° ë¹ ë¥´ë¯€ë¡œ, ìµœì‹  ì˜µì…˜ì€ [ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/en/v0.15.1/models/engine_args.html)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

</details>

---

## 3. ì‹¤ìŠµ: Qwen3-4B-Thinking ì„œë¹™í•˜ê¸°

ì´ì œ ì´ë¡ ì€ ì¶©ë¶„í•©ë‹ˆë‹¤. ì‹¤ì œë¡œ K3s í´ëŸ¬ìŠ¤í„°ì— ë°°í¬í•´ ë´…ì‹œë‹¤.
ì´ë²ˆì— ì‚¬ìš©í•  ëª¨ë¸ì€ `Qwen/Qwen3-4B-Thinking-2507` ì…ë‹ˆë‹¤. (ìµœê·¼ ìœ í–‰í•˜ëŠ” Thinking í”„ë¡œì„¸ìŠ¤ê°€ ë‚´ì¥ëœ 4B ì‚¬ì´ì¦ˆì˜ ëª¨ë¸ì…ë‹ˆë‹¤.)

### 3.1 Deployment Manifest ì‘ì„±

ë¨¼ì € `vllm-qwen3.yaml` íŒŒì¼ì„ ì‘ì„±í•©ë‹ˆë‹¤.
ì§€ë‚œ ì‹œê°„ì— ë§Œë“  `llm-serving` ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ë°°í¬í•˜ê² ìŠµë‹ˆë‹¤.

<details>
<summary>ğŸ“„ <strong>(í´ë¦­) vllm-qwen3.yaml íŒŒì¼ ì „ì²´ ë‚´ìš© ë³´ê¸°</strong></summary>

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-qwen3            # ë””í”Œë¡œì´ë¨¼íŠ¸ ì´ë¦„
  namespace: llm-serving      # ë°°í¬í•  ë„¤ì„ìŠ¤í˜ì´ìŠ¤
  labels:
    app: vllm-qwen3           # ë¼ë²¨ (ì„œë¹„ìŠ¤ì™€ ì—°ê²°í•˜ê¸° ìœ„í•¨)
spec:
  replicas: 1                 # íŒŒë“œ ë³µì œë³¸ ê°œìˆ˜ (GPU 1ê°œë‹¹ 1ê°œê°€ ì ì ˆ)
  selector:
    matchLabels:
      app: vllm-qwen3         # ì–´ë–¤ íŒŒë“œë¥¼ ê´€ë¦¬í• ì§€ ì„ íƒí•˜ëŠ” ë¼ë²¨
  template:
    metadata:
      labels:
        app: vllm-qwen3       # ìƒì„±ë  íŒŒë“œì˜ ë¼ë²¨
    spec:
      runtimeClassName: nvidia  # [ì¤‘ìš”] NVIDIA GPU ì‚¬ìš©ì„ ìœ„í•´ í•„ìˆ˜!
      hostIPC: true             # [ê¶Œì¥] GPU ê°„ ë©”ëª¨ë¦¬ ê³µìœ  ë° ì„±ëŠ¥ ìµœì í™”
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest  # ì‚¬ìš©í•  vLLM ì´ë¯¸ì§€
        imagePullPolicy: IfNotPresent   # ì´ë¯¸ì§€ê°€ ë¡œì»¬ì— ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
        securityContext:
          privileged: true      # [ì°¸ê³ ] ì¼ë¶€ í™˜ê²½ì—ì„œ GPU ì ‘ê·¼ì„ ìœ„í•´ í•„ìš”í•  ìˆ˜ ìˆìŒ
        
        # [ë¦¬ì†ŒìŠ¤ í• ë‹¹]
        resources:
          limits:
            nvidia.com/gpu: 1   # GPU 1ê°œë¥¼ ì „ìš©ìœ¼ë¡œ í• ë‹¹ (í•„ìˆ˜)
            memory: "16Gi"      # ìµœëŒ€ ë©”ëª¨ë¦¬ ì œí•œ
          requests:
            nvidia.com/gpu: 1   # GPU 1ê°œ ìš”ì²­
            memory: "8Gi"       # ìµœì†Œ ë©”ëª¨ë¦¬ ë³´ì¥
            
        # [í™˜ê²½ ë³€ìˆ˜ ì„¤ì •]
        env:
          - name: HUGGING_FACE_HUB_TOKEN
            value: "hf_YOUR_TOKEN_HERE" # (Optional) Gated Model ì‚¬ìš© ì‹œ í† í° í•„ìš”
          - name: VLLM_LOGGING_LEVEL
            value: "INFO"       # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
          - name: VLLM_WORKER_MULTIPROC_METHOD # [ì¤‘ìš”] Worker í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ë°©ì‹ (Python 3.12+ í•„ìˆ˜)
            value: "spawn"
          - name: NCCL_P2P_DISABLE
            value: "1"          # ì†Œë¹„ììš© GPU(RTX ì‹œë¦¬ì¦ˆ)ì—ì„œ P2P ì´ìŠˆ ë°©ì§€
          - name: NCCL_CUMEM_HOST_ENABLE # WSL2/Docker í™˜ê²½ í˜¸í™˜ì„±
            value: "0"
          - name: NCCL_NVLS_ENABLE    # WSL2/Docker í™˜ê²½ í˜¸í™˜ì„±
            value: "0"
          # [í•µì‹¬ - WSL2 í•„ìˆ˜] ì´ë¯¸ì§€ ë‚´ compat ë“œë¼ì´ë²„ ëŒ€ì‹  í˜¸ìŠ¤íŠ¸ ë“œë¼ì´ë²„ë¥¼ ìš°ì„  ë¡œë“œ
          - name: LD_LIBRARY_PATH
            value: "/usr/lib/wsl/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64"
            
        # [ì‹¤í–‰ ì˜µì…˜: command ì—†ì´ argsë§Œ ì‚¬ìš©]
        args:
          # (1) ëª¨ë¸ ì„¤ì •
          # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš© (ë³¼ë¥¨ ë§ˆìš´íŠ¸ ê²½ë¡œ ê¸°ì¤€)
          - --model=/models/Qwen/Qwen3-4B-Thinking-2507
          # HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´: --model=Qwen/Qwen3-4B-Thinking-2507
          - --served-model-name=qwen3-4b  # API í˜¸ì¶œ ì‹œ ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ ë³„ì¹­
          
          # (2) GPU ë° ë©”ëª¨ë¦¬ ì„¤ì •
          - --gpu-memory-utilization=0.85  # VRAMì˜ 85%ë¥¼ KV ìºì‹œë¡œ ì‚¬ìš©
          - --max-model-len=8192          # ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (OOM ë°©ì§€)
          - --dtype=auto                  # ë°ì´í„° íƒ€ì… ìë™ ì„¤ì • (BF16 ë“±)
          
          # (3) ê¸°íƒ€ ì„¤ì •
          - --trust-remote-code           # ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ëª¨ë¸ ì‚¬ìš© ì‹œ í•„ìš”
          
        # [í¬íŠ¸ ì„¤ì •]
        ports:
        - containerPort: 8000   # vLLM ê¸°ë³¸ í¬íŠ¸
          name: http
          
        # [ìƒíƒœ ê²€ì‚¬ (Health Check)]
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120 # ëª¨ë¸ ë¡œë”© ì‹œê°„ ë™ì•ˆ ëŒ€ê¸° (ë„‰ë„‰í•˜ê²Œ)
          periodSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 5
          failureThreshold: 3
        startupProbe:             # ì´ˆê¸° ê¸°ë™ ì‹œì—ë§Œ ì²´í¬ (ì„±ê³µí•  ë•Œê¹Œì§€ liveness ì‹¤íŒ¨ ë¬´ì‹œ)
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
          failureThreshold: 30    # 10ì´ˆ*30íšŒ = ìµœëŒ€ 5ë¶„ê¹Œì§€ ëŒ€ê¸°
          
        # [ë³¼ë¥¨ ë§ˆìš´íŠ¸]
        volumeMounts:
          - name: hf-cache
            mountPath: /root/.cache/huggingface # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ìºì‹œ ê²½ë¡œ
          - name: models-volume
            mountPath: /models                  # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ
            readOnly: true
            
      # [ë³¼ë¥¨ ì •ì˜]
      volumes:
      - name: hf-cache
        hostPath:
          path: /root/.cache/huggingface        # í˜¸ìŠ¤íŠ¸(WSL2)ì˜ ìºì‹œ ê²½ë¡œ ê³µìœ 
          type: DirectoryOrCreate
      - name: models-volume
        hostPath:
          # [ì¤‘ìš”] ë¡œì»¬ ëª¨ë¸ì´ ìˆëŠ” ì‹¤ì œ í˜¸ìŠ¤íŠ¸ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”!
          path: /mnt/c/Users/takeaim/models 
          type: DirectoryOrCreate
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-qwen3-service    # ì„œë¹„ìŠ¤ ì´ë¦„
  namespace: llm-serving
spec:
  selector:
    app: vllm-qwen3           # ì—°ê²°í•  íŒŒë“œ ë¼ë²¨
  ports:
    - protocol: TCP
      port: 80                # ì™¸ë¶€ì—ì„œ ì ‘ì†í•  í¬íŠ¸
      targetPort: 8000        # ë‚´ë¶€ íŒŒë“œ í¬íŠ¸ (vLLM)
  type: ClusterIP             # í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ IP í• ë‹¹
```

</details>

#### ğŸ“„ YAML íŒŒì¼ ìƒì„¸ ì„¤ëª… (ì´ˆë³´ì í•„ë…!)

ì´ì „ í¬ìŠ¤íŒ…ì—ì„œ `Deployment`ì™€ `Service`ì˜ ê°œë…ë§Œ ê°„ë‹¨íˆ ì§šê³  ë„˜ì–´ê°”ëŠ”ë°, ìœ„ ì„¤ì • íŒŒì¼ì˜ ê° ë¶€ë¶„ì´ ì–´ë–¤ ì˜ë¯¸ì¸ì§€ ìƒì„¸íˆ ì•Œì•„ë´…ì‹œë‹¤.

*   `runtimeClassName: nvidia`: **ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤!** K3sê°€ íŒŒë“œë¥¼ ë§Œë“¤ ë•Œ NVIDIA ì»¨í…Œì´ë„ˆ ëŸ°íƒ€ì„ì„ ì‚¬ìš©í•˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤. ì´ê²Œ ì—†ìœ¼ë©´ GPUê°€ ìˆì–´ë„ ì¸ì‹í•˜ì§€ ëª»í•©ë‹ˆë‹¤.
*   `args`: vLLM ì‹¤í–‰ ì˜µì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤. `--key=value` í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.
*   `--model`: ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤. **ë¡œì»¬ ëª¨ë¸**ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë³¼ë¥¨ ë§ˆìš´íŠ¸ ê²½ë¡œ(ì˜ˆ: `--model=/models/...`)ë¡œ, **HuggingFace**ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´ ëª¨ë¸ ID(ì˜ˆ: `--model=Qwen/Qwen3-4B-Thinking-2507`)ë¡œ ì§€ì •í•©ë‹ˆë‹¤. ë¡œì»¬ ëª¨ë¸ì„ ì“°ë©´ ë‹¤ìš´ë¡œë“œ ì‹œê°„ì„ ìƒëµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
*   `LD_LIBRARY_PATH`: **(WSL2 í•„ìˆ˜)** vLLM ì´ë¯¸ì§€ ë‚´ì˜ compat ë“œë¼ì´ë²„ ëŒ€ì‹  WSL2 í˜¸ìŠ¤íŠ¸ ë“œë¼ì´ë²„ê°€ ë¨¼ì € ë¡œë”©ë˜ë„ë¡ ê²½ë¡œë¥¼ ìš°ì„ ì‹œí•©ë‹ˆë‹¤. ì´ ì„¤ì •ì´ ì—†ìœ¼ë©´ `No CUDA GPUs are available` ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.
*   `livenessProbe` / `readinessProbe` / `startupProbe`: ì¿ ë²„ë„¤í‹°ìŠ¤ê°€ íŒŒë“œì˜ ìƒíƒœë¥¼ ì²´í¬í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. `startupProbe`ëŠ” ì´ˆê¸° ê¸°ë™ ì‹œì—ë§Œ ì²´í¬í•˜ë©°, ì´ í”„ë¡œë¸Œê°€ ì„±ê³µí•  ë•Œê¹Œì§€ `livenessProbe` ì‹¤íŒ¨ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.
*   `volumeMounts` & `volumes`:
    *   **hf-cache**: í˜¸ìŠ¤íŠ¸(`WSL2 Ubuntu`)ì˜ ìºì‹œ í´ë”ë¥¼ ê³µìœ í•˜ì—¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œê°„ì„ ë‹¨ì¶•í•©ë‹ˆë‹¤.
    *   **models-volume**: ë¡œì»¬ì— ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ì´ ìˆë‹¤ë©´, í•´ë‹¹ ê²½ë¡œë¥¼ ë§ˆìš´íŠ¸í•˜ì—¬ ì¸í„°ë„· ë‹¤ìš´ë¡œë“œ ì—†ì´ ì¦‰ì‹œ ë¡œë”©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (`--model=/models/...`ë¡œ ê²½ë¡œ ì§€ì • í•„ìš”)

### 3.2 ë°°í¬ ë° ë¡œê·¸ í™•ì¸

ì‘ì„±í•œ YAML íŒŒì¼ì„ K3s í´ëŸ¬ìŠ¤í„°ì— ì ìš©í•©ë‹ˆë‹¤.

```bash
# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ê°€ ì—†ë‹¤ë©´ ë¨¼ì € ìƒì„±
kubectl create namespace llm-serving

# ë°°í¬ ì ìš©
kubectl apply -f vllm-qwen3.yaml
```

#### ğŸš§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…: íŒŒë“œê°€ ëœ¨ì§€ ì•Šë‚˜ìš”?

GPUê°€ 1ê°œë¿ì¸ í™˜ê²½ì—ì„œëŠ” ìì› ê²½í•©ìœ¼ë¡œ ì¸í•´ ìƒˆ íŒŒë“œê°€ ë°”ë¡œ ì‹¤í–‰ë˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> **ì¦ìƒ 1: ìì› ë¶€ì¡± (Pending)**
> ê¸°ì¡´ íŒŒë“œê°€ ì™„ì „íˆ ì¢…ë£Œë˜ê¸° ì „ê¹Œì§€ ìƒˆ íŒŒë“œëŠ” GPUë¥¼ í• ë‹¹ë°›ì§€ ëª»í•´ `Pending` ìƒíƒœë¡œ ëŒ€ê¸°í•©ë‹ˆë‹¤.

> **ì¦ìƒ 2: GPU ì¸ì‹ ì‹¤íŒ¨ (CrashLoopBackOff)**
> ë¡œê·¸ì— `RuntimeError: No CUDA GPUs are available` ì—ëŸ¬ê°€ ëœ¨ëŠ” ê²½ìš°ì…ë‹ˆë‹¤.
>
> ì´ ì—ëŸ¬ëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ ì›ì¸ìœ¼ë¡œ ë°œìƒí•©ë‹ˆë‹¤:
> 1. **CDI ìŠ¤í™ ë¯¸ìƒì„±**: [K3s ì„¤ì¹˜ ê°€ì´ë“œ](./2026-02-07-k3s-wsl-install/index.md)ì˜ 1ë‹¨ê³„ì—ì„œ `sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml` ëª…ë ¹ì„ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
> 2. **CUDA compat ë“œë¼ì´ë²„ ì¶©ëŒ**: YAMLì˜ `env`ì— `LD_LIBRARY_PATH: "/usr/lib/wsl/lib:..."` ì„¤ì •ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. (WSL2 í™˜ê²½ í•„ìˆ˜)

> **ì¦ìƒ 3: ë¬´í•œ ì¬ì‹œì‘ (CrashLoopBackOff) - ê¸°ì¡´ íŒŒë“œ ì¶©ëŒ**
> ê¸°ì¡´ íŒŒë“œê°€ ì—ëŸ¬ë¡œ ì¸í•´ ê³„ì† ì¬ì‹œì‘(`Running` -> `Error` -> `CrashLoopBackOff`)ë˜ëŠ” ê²½ìš°ì—ë„ GPUë¥¼ ì ìœ í•˜ê³  ìˆì–´, ìƒˆ íŒŒë“œëŠ” ì‹¤í–‰ë˜ì§€ ëª»í•©ë‹ˆë‹¤.
> **í•´ê²° ë°©ë²•:** ë¬¸ì œê°€ ìˆëŠ” ê¸°ì¡´ íŒŒë“œë¥¼ **ìˆ˜ë™ìœ¼ë¡œ ì‚­ì œ**í•˜ì„¸ìš”.


```bash
# 1. íŒŒë“œ ëª©ë¡ í™•ì¸
kubectl get pods -n llm-serving

# 2. ë¬¸ì œ ìˆê±°ë‚˜ ì¢…ë£Œë˜ì§€ ì•Šì€ ê¸°ì¡´ íŒŒë“œ ì‚­ì œ (ì´ë¦„ì„ í™•ì¸í•´ì„œ ë„£ìœ¼ì„¸ìš”)
kubectl delete pod vllm-qwen3-xxxx... -n llm-serving
```

ë§Œì•½ ìƒíƒœê°€ ë„ˆë¬´ ë³µì¡í•´ì„œ í•´ê²°ì´ ì•ˆ ëœë‹¤ë©´, ì•„ì˜ˆ ì‹¹ ì§€ìš°ê³  ë‹¤ì‹œ ì‹œì‘í•˜ëŠ” ê²ƒë„ ë°©ë²•ì…ë‹ˆë‹¤.

```bash
# ìƒì„±í–ˆë˜ ìì›(Deployment, Service) ëª¨ë‘ ì‚­ì œ (Reset)
kubectl delete -f vllm-qwen3.yaml

# (ì ì‹œ í›„ íŒŒë“œê°€ ì‚¬ë¼ì§„ ê²ƒì„ í™•ì¸í•˜ê³ ) ë‹¤ì‹œ ë°°í¬
kubectl apply -f vllm-qwen3.yaml
```

<details>
<summary>ğŸ”½ <strong>(í´ë¦­) 'kubectl apply'ë¥¼ í•˜ë©´ ë‚´ë¶€ì—ì„œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ë‚˜ìš”?</strong></summary>

ì´ ê³¼ì •ì„ ì´í•´í•˜ë©´ ë””ë²„ê¹… ì‹¤ë ¥ì´ ì‘¥ì‘¥ ëŠ¡ë‹ˆë‹¤!

1.  **API ì„œë²„ ìš”ì²­**: `kubectl`ì´ YAML ë‚´ìš©ì„ K3s API ì„œë²„ì— ë³´ëƒ…ë‹ˆë‹¤ ("ì´ëŒ€ë¡œ ë§Œë“¤ì–´ì¤˜!").
2.  **Deployment ìƒì„±**: Deployment ì»¨íŠ¸ë¡¤ëŸ¬ê°€ ì´ë¥¼ ì ‘ìˆ˜í•˜ê³ , "íŒŒë“œ 1ê°œë¥¼ ë§Œë“¤ì–´ì•¼ê² êµ°" í•˜ê³  ReplicaSetì„ ë§Œë“­ë‹ˆë‹¤.
3.  **Scheduler í• ë‹¹**: ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ë“±ì¥í•©ë‹ˆë‹¤. "GPUê°€ í•„ìš”í•œ íŒŒë“œë„¤? GPUê°€ ìˆëŠ” ë…¸ë“œê°€ ì–´ë””ì§€?" í•˜ê³  ì ì ˆí•œ ë…¸ë“œì— íŒŒë“œë¥¼ ë°°ì •í•©ë‹ˆë‹¤.
4.  **Kubelet ì‹¤í–‰**: í•´ë‹¹ ë…¸ë“œì˜ Kubeletì´ ì§€ì‹œë¥¼ ë°›ê³ , ì»¨í…Œì´ë„ˆ ëŸ°íƒ€ì„(containerd)ì—ê²Œ ì»¨í…Œì´ë„ˆ ìƒì„±ì„ ëª…ë ¹í•©ë‹ˆë‹¤.
5.  **ì´ë¯¸ì§€ í’€(Pull)**: `vllm/vllm-openai:latest` ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ë°›ìŠµë‹ˆë‹¤. (ì‹œê°„ì´ ì¢€ ê±¸ë¦¼)
6.  **ì»¨í…Œì´ë„ˆ ì‹œì‘**: ì´ë¯¸ì§€ê°€ ì¤€ë¹„ë˜ë©´ ì»¨í…Œì´ë„ˆë¥¼ ë„ìš°ê³ , ìš°ë¦¬ê°€ ì •ì˜í•œ `args` ì˜µì…˜ìœ¼ë¡œ vLLMì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
7.  **vLLM ì´ˆê¸°í™”**: vLLM í”„ë¡œì„¸ìŠ¤ê°€ ëœ¨ë©´ì„œ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ê³ , GPU ë©”ëª¨ë¦¬ë¥¼ í™•ë³´í•˜ê³ , ì¶”ë¡  ì¤€ë¹„ë¥¼ ë§ˆì¹©ë‹ˆë‹¤.
8.  **Ready ìƒíƒœ**: ëª¨ë“  ì¤€ë¹„ê°€ ëë‚˜ë©´ íŒŒë“œ ìƒíƒœê°€ `Running`ì´ ë˜ê³ , `Ready` ì¡°ê±´ì´ ë§Œì¡±ë©ë‹ˆë‹¤.

</details>

#### ğŸ” ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ (ê°€ì¥ ì¤‘ìš”!)

ë°°í¬ ì§í›„ì—ëŠ” íŒŒë“œ ìƒíƒœê°€ `ContainerCreating` ì¼ ê²ƒì…ë‹ˆë‹¤. vLLM ì´ë¯¸ì§€ê°€ ì›Œë‚™ í¬ê¸° ë•Œë¬¸ì—(ëª‡ GB) ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ê½¤ ê±¸ë¦½ë‹ˆë‹¤.

```bash
# [-w] ì˜µì…˜ìœ¼ë¡œ ìƒíƒœ ë³€í™”ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì§€ì¼œë´…ë‹ˆë‹¤.
kubectl get pods -n llm-serving -w
```

íŒŒë“œê°€ `Running` ìƒíƒœê°€ ë˜ë©´, ì‹¤ì œë¡œ ëª¨ë¸ì´ ë¡œë”©ë˜ê³  ìˆëŠ”ì§€ ë¡œê·¸ë¥¼ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.

```bash
# Pod ì´ë¦„ìœ¼ë¡œ ë¡œê·¸ í™•ì¸
kubectl logs -f vllm-qwen3-xxxx... -n llm-serving
# ë¼ë²¨ì„ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ í™•ì¸ (ì¶”ì²œ)
kubectl logs -f -l app=vllm-qwen3 -n llm-serving
```

**ì„±ê³µ ë¡œê·¸ ì˜ˆì‹œ:**
ë¡œê·¸ ë§ˆì§€ë§‰ì— ë‹¤ìŒê³¼ ê°™ì€ ë¬¸êµ¬ê°€ ë– ì•¼ ì„±ê³µì…ë‹ˆë‹¤.

```
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

ë§Œì•½ ì¤‘ê°„ì— `OOM(Out Of Memory)` ì—ëŸ¬ê°€ ëœ¨ê±°ë‚˜ í”„ë¡œì„¸ìŠ¤ê°€ ì£½ëŠ”ë‹¤ë©´, `gpu-memory-utilization` ê°’ì„ ì¡°ê¸ˆ ì¤„ì´ê±°ë‚˜ `max-model-len`ì„ ì¤„ì—¬ì„œ ë‹¤ì‹œ `apply` í•´ë³´ì„¸ìš”. ( DeploymentëŠ” ë‚´ìš©ì´ ë°”ë€Œë©´ ì•Œì•„ì„œ íŒŒë“œë¥¼ ì¬ì‹œì‘í•´ì¤ë‹ˆë‹¤! ğŸ‘ )

---

## 4. API í…ŒìŠ¤íŠ¸: Thinking í”„ë¡œì„¸ìŠ¤ í™•ì¸

vLLMì€ OpenAI í˜¸í™˜ APIë¥¼ ì œê³µí•˜ë¯€ë¡œ, ê¸°ì¡´ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
í¬íŠ¸ í¬ì›Œë”©ì„ í†µí•´ ë¡œì»¬ì—ì„œ APIë¥¼ í˜¸ì¶œí•´ ë³´ê² ìŠµë‹ˆë‹¤.

### 4.1 í¬íŠ¸ í¬ì›Œë”© ì„¤ì •

```bash
# ë¡œì»¬ í¬íŠ¸ 8000ë²ˆì„ ì„œë¹„ìŠ¤ í¬íŠ¸ 80ë²ˆì— ì—°ê²°
kubectl port-forward svc/vllm-qwen3-service 8000:80 -n llm-serving
```
ì´ì œ í„°ë¯¸ë„ì„ í•˜ë‚˜ ë” ì—´ì–´ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

### 4.2 ê°„ë‹¨í•œ API í…ŒìŠ¤íŠ¸ (Python One-liner)

íŒŒì¼ì„ ë”°ë¡œ ë§Œë“¤ í•„ìš” ì—†ì´, í„°ë¯¸ë„ì—ì„œ íŒŒì´ì¬ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬(`urllib`)ë¥¼ ì´ìš©í•´ ë°”ë¡œ ê²°ê³¼ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.
Thinking ëª¨ë¸ì˜ íŠ¹ì§•ì¸ ì‚¬ê³  ê³¼ì •(`<think>...</think>`)ì´ ì¶œë ¥ë˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# ì•„ë˜ ëª…ë ¹ì–´ë¥¼ í„°ë¯¸ë„ì— ë³µì‚¬í•´ì„œ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.
python3 -c "import urllib.request, json; \
print(json.load(urllib.request.urlopen(urllib.request.Request( \
    'http://localhost:8000/v1/chat/completions', \
    data=json.dumps({ \
        'model': 'qwen3-4b', \
        'messages': [{'role': 'user', 'content': '3.11ê³¼ 3.9 ì¤‘ ì–´ëŠ ìˆ«ìê°€ ë” í°ê°€ìš”?'}], \
        'temperature': 0.7 \
    }).encode('utf-8'), \
    headers={'Content-Type': 'application/json'} \
)))['choices'][0]['message']['content'])"
```

**[ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ]**

```
"
<think>
Okay, let me try to figure out which number is bigger between 3.11 and 3.9. Hmm, so both numbers start with 3, right? So the whole number part is the same for both. That means I need to look at the decimal parts to tell which one is larger.

Let me write them out: 3.11 and 3.9. Wait, 3.9 is the same as 3.90 if I add a zero at the end to make it two decimal places, right? So comparing 3.11 and 3.90.

In decimal numbers, the first digit after the decimal point is the tenths place. So for 3.11, the tenths digit is 1, and for 3.9, the tenths digit is 9. Since 9 is greater than 1, that would make 3.9 larger than 3.11.

Wait, but maybe I should check if there's a trick here. Sometimes people get confused with the number of decimal places. Let me think. For example, 3.11 is three and eleven hundredths, which is 3 + 11/100. And 3.9 is three and nine tenths, which is 3 + 9/10. Converting 9/10 to hundredths would be 90/100. So 90/100 is bigger than 11/100. Therefore, 3.9 is bigger than 3.11.

Another way to think about it: if I subtract them, 3.9 - 3.11 = 0.79, which is positive, so 3.9 is larger.

Wait, but maybe someone might think that because 3.11 has two decimal places and 3.9 has one, so 3.11 is more precise? But no, when comparing the numbers, the number of decimal places doesn't matter as much as the actual values. Even though 3.9 is written with one decimal, it's equivalent to 3.90, so it's 3.90 which is larger than 3.11.

Let me visualize them on a number line. Starting from 3, the next number is 3.1, then 3.2, up to 3.9. So 3.11 is just a bit past 3.1, whereas 3.9 is almost 4. So yeah, 3.9 is definitely bigger.

I think that's solid. So the answer should be 3.9 is larger than 3.11.
</think>

3.9ì€ 3.11ë³´ë‹¤ ë” í¬ìŠµë‹ˆë‹¤.
**ê·¼ê±°**:
- ë‘ ìˆ˜ì˜ ì •ìˆ˜ ë¶€ë¶„(3)ì€ ë™ì¼í•˜ë¯€ë¡œ, ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•©ë‹ˆë‹¤.
- 3.9ëŠ” 3.90ê³¼ ë™ì¼í•˜ë©°, ì†Œìˆ˜ ì²«ì§¸ ìë¦¬(9)ê°€ 3.11ì˜ ì†Œìˆ˜ ì²«ì§¸ ìë¦¬(1)ë³´ë‹¤ í¬ê¸° ë•Œë¬¸ì— **3.9 > 3.11**ì…ë‹ˆë‹¤.

**ì •ë‹µ**: 3.9ê°€ ë” í½ë‹ˆë‹¤.
...
```

ëª¨ë¸ì´ ë‹µë³€ì„ ë‚´ë†“ê¸° ì „ì— `<think>` íƒœê·¸ ì•ˆì—ì„œ ìŠ¤ìŠ¤ë¡œ ì¶”ë¡ í•˜ëŠ” ê³¼ì •ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## 5. ë§ˆì¹˜ë©°: ë‹¤ìŒ ì˜ˆê³ 

ì˜¤ëŠ˜ì€ **vLLM**ì„ í™œìš©í•´ ê³ ì„±ëŠ¥ ì¶”ë¡  í™˜ê²½ì„ êµ¬ì¶•í•˜ê³ , ìµœì‹  Thinking ëª¨ë¸ì„ ì„œë¹™í•˜ëŠ” ë°©ë²•ê¹Œì§€ ì•Œì•„ë´¤ìŠµë‹ˆë‹¤.
íŠ¹íˆ `Deployment`ì™€ `Service`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿ ë²„ë„¤í‹°ìŠ¤ í™˜ê²½ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ìš´ì˜í•  ìˆ˜ ìˆëŠ” ê¸°ë°˜ì„ ë‹¦ì•˜ìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ, "ë‚˜ëŠ” ë³µì¡í•œ ì„¤ì • ì—†ì´ ê·¸ëƒ¥ ë¡œì»¬ì—ì„œ ê°„ë‹¨í•˜ê²Œ ëª¨ë¸ í•œë²ˆ ëŒë ¤ë³´ê³  ì‹¶ì–´!" í•˜ëŠ” ë¶„ë“¤ë„ ê³„ì‹œê² ì£ ?
ê·¸ëŸ° ë¶„ë“¤ì„ ìœ„í•´ **2íƒ„**ì—ì„œëŠ” **Ollama**ë¥¼ í™œìš©í•œ ì´ˆê°„ë‹¨ ì„œë¹™ ë°©ë²•ì„ ì†Œê°œí•˜ê² ìŠµë‹ˆë‹¤.

> **Next Post**: ğŸ¦™ **Ollama**ë¡œ ë¡œì»¬ LLM ì •ë³µí•˜ê¸° (feat. Modelfile ì»¤ìŠ¤í„°ë§ˆì´ì§•)

ê¸°ëŒ€í•´ ì£¼ì„¸ìš”! ğŸš€
