---
slug: k3s-wsl-install-guide
title: "[Infra] WSL2 + K3s + NVIDIA GPU ì™„ì „ ì •ë³µ ê°€ì´ë“œ"
authors: [me]
tags: [k3s, wsl, gpu, nvidia, infra]
---

ì´ ë¬¸ì„œëŠ” **Windows 11 (WSL2 Ubuntu)** í™˜ê²½ì—ì„œ **K3s**ë¥¼ ì„¤ì¹˜í•˜ê³ , **NVIDIA GPU**ë¥¼ íŒŒë“œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•˜ëŠ” ìµœì¢… ì •ë¦¬ë³¸ì…ë‹ˆë‹¤.

> âš ï¸ **ì¤‘ìš”**: ì´ ê°€ì´ë“œëŠ” **K3s v1.34.x** ë²„ì „ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ë²„ì „ì— ë”°ë¼ containerd ì„¤ì • í˜•ì‹ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### âœ… ì „ì œ ì¡°ê±´

1. **Windows í˜¸ìŠ¤íŠ¸**ì— ìµœì‹  NVIDIA ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
2. **WSL2 í„°ë¯¸ë„**ì—ì„œ `nvidia-smi` ëª…ë ¹ì–´ê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
3. ì¼ë¶€ ì‘ì—…ì— **sudo ê¶Œí•œ**ì´ í•„ìš”í•©ë‹ˆë‹¤. (ì¼ë°˜ ì‚¬ìš©ìë¡œë„ ìˆ˜í–‰ ê°€ëŠ¥)

---

## 0ë‹¨ê³„: kubectl alias ì„¤ì • (ê¶Œì¥)

K3s ì„¤ì¹˜ ì‹œ `/usr/local/bin/kubectl` ì‹¬ë³¼ë¦­ ë§í¬ê°€ ìƒì„±ë©ë‹ˆë‹¤. ë§Œì•½ `kubectl` ëª…ë ¹ì–´ê°€ ì‘ë™í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ aliasë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

```bash
# ~/.bashrc ë˜ëŠ” ~/.zshrcì— ì¶”ê°€
echo 'alias kubectl="k3s kubectl"' >> ~/.bashrc
source ~/.bashrc
```

> **ì°¸ê³ **: ì•„ë˜ ëª…ë ¹ì–´ë“¤ì€ alias ì„¤ì •ì´ ë˜ì–´ìˆë‹¤ëŠ” ê°€ì • í•˜ì— `kubectl`ë¡œ í‘œê¸°í•©ë‹ˆë‹¤. alias ì„¤ì •ì´ ì•ˆ ë˜ì–´ ìˆë‹¤ë©´ `k3s kubectl`ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.

---

## 1ë‹¨ê³„: NVIDIA Container Toolkit ì„¤ì¹˜

K3sê°€ GPUë¥¼ ì¸ì‹í•˜ë ¤ë©´ **NVIDIA ì»¨í…Œì´ë„ˆ íˆ´í‚·**ì´ í•„ìš”í•©ë‹ˆë‹¤.

```bash
# 1. NVIDIA Container Toolkit ì €ì¥ì†Œ ì¶”ê°€
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/libnvidia-container/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | sudo tee /etc/apt/sources.list.d/libnvidia-container.list

# 2. íˆ´í‚· ì„¤ì¹˜
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# 3. [WSL2 í•„ìˆ˜] CDI(Container Device Interface) ìŠ¤í™ ìƒì„±
#    ì´ ë‹¨ê³„ë¥¼ ë¹ ëœ¨ë¦¬ë©´, ë‚˜ì¤‘ì— vLLM ë“± GPU ì›Œí¬ë¡œë“œì—ì„œ
#    "No CUDA GPUs are available" ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.
sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
```

> **âš ï¸ ì¤‘ìš” (2026-02-15 ì¶”ê°€)**
> WSL2 í™˜ê²½ì—ì„œëŠ” ë°˜ë“œì‹œ `nvidia-ctk cdi generate` ëª…ë ¹ìœ¼ë¡œ **CDI ìŠ¤í™ íŒŒì¼**ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
> CDI ìŠ¤í™ì´ ì—†ìœ¼ë©´ `nvidia-container-runtime`ì´ GPU ë””ë°”ì´ìŠ¤(`/dev/dxg`)ë¥¼ ì»¨í…Œì´ë„ˆì— ì˜¬ë°”ë¥´ê²Œ ì£¼ì…í•˜ì§€ ëª»í•©ë‹ˆë‹¤.
> ì´ ëª…ë ¹ì–´ëŠ” **ìµœì´ˆ 1íšŒë§Œ** ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤. (GPU ë“œë¼ì´ë²„ë¥¼ ì—…ë°ì´íŠ¸í•œ ê²½ìš° ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.)

---

## 2ë‹¨ê³„: K3s ì„¤ì¹˜

ë¨¼ì € K3së¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. K3sê°€ ì‹œì‘ë˜ë©´ ê¸°ë³¸ containerd ì„¤ì • íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤.

```bash
curl -sfL https://get.k3s.io | sh -

# (ì¼ë°˜ ì‚¬ìš©ìì¸ ê²½ìš°) kubeconfig íŒŒì¼ ì½ê¸° ê¶Œí•œ ë¶€ì—¬
sudo chmod 644 /etc/rancher/k3s/k3s.yaml

# ë…¸ë“œê°€ Ready ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ì•½ 30ì´ˆ-1ë¶„)
sleep 30
k3s kubectl get nodes
```

> **âš ï¸ ì£¼ì˜**: K3sëŠ” ê¸°ë³¸ì ìœ¼ë¡œ kubeconfig íŒŒì¼ì„ rootë§Œ ì½ì„ ìˆ˜ ìˆë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤. ì¼ë°˜ ì‚¬ìš©ìë¡œ kubectlì„ ì‚¬ìš©í•˜ë ¤ë©´ ìœ„ì™€ ê°™ì´ ê¶Œí•œì„ ë³€ê²½í•˜ê±°ë‚˜, K3s ì„¤ì¹˜ ì‹œ `--write-kubeconfig-mode 644` ì˜µì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”.

---

## 3ë‹¨ê³„: containerd ì„¤ì • í…œí”Œë¦¿ ìƒì„± (í•µì‹¬!)

K3sê°€ ìƒì„±í•œ ê¸°ë³¸ config.tomlì„ ë³µì‚¬í•˜ì—¬ **nvidia runtimeì„ ê¸°ë³¸ìœ¼ë¡œ** ì„¤ì •í•©ë‹ˆë‹¤.

```bash
# 1. K3sê°€ ìƒì„±í•œ ê¸°ë³¸ config.tomlì„ í…œí”Œë¦¿ìœ¼ë¡œ ë³µì‚¬
sudo cp /var/lib/rancher/k3s/agent/etc/containerd/config.toml /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl

# 2. default_runtime_name = "nvidia" ì„¤ì • ì¶”ê°€
sudo sed -i "/\[plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.runc\]/i [plugins.'io.containerd.cri.v1.runtime'.containerd]\n  default_runtime_name = \"nvidia\"\n" /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl

# 3. K3s ì¬ì‹œì‘í•˜ì—¬ ì„¤ì • ì ìš©
sudo systemctl restart k3s

# 4. (ì¼ë°˜ ì‚¬ìš©ìì¸ ê²½ìš°) kubeconfig ê¶Œí•œ ì¬ì„¤ì • (K3s ì¬ì‹œì‘ ì‹œ ë¦¬ì…‹ë¨)
sudo chmod 644 /etc/rancher/k3s/k3s.yaml

# 5. ë…¸ë“œê°€ Ready ìƒíƒœì¸ì§€ í™•ì¸
sleep 20
k3s kubectl get nodes
```

> **ğŸ’¡ ì°¸ê³ **: K3s v1.34+ ë²„ì „ì—ì„œëŠ” containerd v3 ì„¤ì • í˜•ì‹(`plugins.'io.containerd.cri.v1.runtime'`)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ì „ ë²„ì „(v1.28 ì´í•˜)ì—ì„œëŠ” `plugins.cri` í˜•ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## 4ë‹¨ê³„: WSL2 ì „ìš© NVIDIA Device Plugin ë°°í¬ (í•µì‹¬)

ê¸°ë³¸ Helm ì°¨íŠ¸ì˜ ì„¤ì •ê°’ì€ WSL2 í™˜ê²½ì— ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. **WSL2ì˜ íŠ¹ìˆ˜ ê²½ë¡œ(`/dev/dxg`, `/usr/lib/wsl`)ì™€ runtimeClassNameì´ ì ìš©ëœ ì»¤ìŠ¤í…€ YAML**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

```bash
# ì•„ë˜ ë‚´ìš©ì„ í†µì§¸ë¡œ ë³µì‚¬í•´ì„œ í„°ë¯¸ë„ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-device-plugin-daemonset
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: nvidia-device-plugin-ds
  template:
    metadata:
      labels:
        name: nvidia-device-plugin-ds
    spec:
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      priorityClassName: system-node-critical
      # [K3s í•µì‹¬] nvidia runtime ì‚¬ìš© ëª…ì‹œ
      runtimeClassName: nvidia
      containers:
      - image: nvcr.io/nvidia/k8s-device-plugin:v0.17.0
        name: nvidia-device-plugin-ctr
        securityContext:
          privileged: true
        env:
          - name: FAIL_ON_INIT_ERROR
            value: "false"
          # [WSL2 í•„ìˆ˜] ë“œë¼ì´ë²„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ê°•ì œ ì§€ì •
          - name: LD_LIBRARY_PATH
            value: "/usr/lib/wsl/lib"
          # [WSL2 í•„ìˆ˜] ëª¨ë“  GPU ë° ë“œë¼ì´ë²„ ê¸°ëŠ¥ í™œì„±í™”
          - name: NVIDIA_VISIBLE_DEVICES
            value: "all"
          - name: NVIDIA_DRIVER_CAPABILITIES
            value: "all"
        volumeMounts:
          - name: device-plugin
            mountPath: /var/lib/kubelet/device-plugins
          - name: wsl
            mountPath: /usr/lib/wsl
            readOnly: true
          - name: dxg
            mountPath: /dev/dxg
      volumes:
        - name: device-plugin
          hostPath:
            path: /var/lib/kubelet/device-plugins
        - name: wsl
          hostPath:
            path: /usr/lib/wsl
        - name: dxg
          hostPath:
            path: /dev/dxg
EOF
```

> **âš ï¸ ë³€ê²½ì‚¬í•­ (2026-02-08 ê²€ì¦)**:
> - `runtimeClassName: nvidia` ì¶”ê°€ - K3sì—ì„œ nvidia runtimeì„ ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©
> - ì´ë¯¸ì§€ ë²„ì „ `v0.17.0`ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ - ìµœì‹  GPU ì§€ì› ë° ì•ˆì •ì„± ê°œì„ 
> - `DEVICE_LIST_STRATEGY`, `DEVICE_ID_STRATEGY` í™˜ê²½ë³€ìˆ˜ ì œê±° - v0.17.0ì—ì„œ ìë™ ê°ì§€

---

## 5ë‹¨ê³„: ìµœì¢… ê²€ì¦ (Verification)

ì„¤ì¹˜ í›„ ì•½ 30ì´ˆ~1ë¶„ ë’¤ì— ì•„ë˜ ëª…ë ¹ì–´ë¡œ í™•ì¸í•©ë‹ˆë‹¤.

### 1. Device Plugin ìƒíƒœ í™•ì¸

```bash
kubectl get pods -n kube-system | grep nvidia
```

> **ì„±ê³µ ê¸°ì¤€:** Pod ìƒíƒœê°€ `Running`ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

### 2. ë…¸ë“œ ì¸ì‹ í™•ì¸

ë…¸ë“œ ìƒì„¸ ì •ë³´ì—ì„œ GPUê°€ ë³´ì—¬ì•¼ í•©ë‹ˆë‹¤.

```bash
kubectl get node -o jsonpath='{.items[0].status.capacity}' | tr ',' '\n'
```

> **ì„±ê³µ ê¸°ì¤€:** ì¶œë ¥ ê²°ê³¼ì— `"nvidia.com/gpu":"1"`ì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

ë˜ëŠ”:

```bash
kubectl describe node | grep "Allocatable" -A 10
```

### 3. ì‹¤ì œ ë™ì‘ í…ŒìŠ¤íŠ¸ (Smoke Test)

ì‹¤ì œ íŒŒë“œë¥¼ ìƒì„±í•˜ì—¬ `nvidia-smi` ëª…ë ¹ì–´ê°€ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

```bash
# í…ŒìŠ¤íŠ¸ íŒŒë“œ ìƒì„±
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: gpu-test
spec:
  restartPolicy: Never
  # [K3s í•µì‹¬] nvidia runtime ì‚¬ìš© ëª…ì‹œ
  runtimeClassName: nvidia
  containers:
  - name: cuda-container
    image: nvidia/cuda:12.3.1-base-ubuntu22.04
    command: ["nvidia-smi"]
    resources:
      limits:
        nvidia.com/gpu: 1
EOF

# (ì ì‹œ ëŒ€ê¸° í›„) ë¡œê·¸ í™•ì¸
sleep 20
kubectl logs gpu-test
```

> **ì„±ê³µ ê¸°ì¤€:** ë¡œê·¸ì— í˜¸ìŠ¤íŠ¸ì™€ ë™ì¼í•œ **GPU í‘œ(ì˜ˆ: RTX 5060 Ti)**ê°€ ì¶œë ¥ë˜ë©´ ì„±ê³µì…ë‹ˆë‹¤.

### 4. í…ŒìŠ¤íŠ¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬

ê²€ì¦ì´ ì™„ë£Œë˜ë©´ í…ŒìŠ¤íŠ¸ìš© íŒŒë“œëŠ” ì‚­ì œí•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.

```bash
kubectl delete pod gpu-test
```

> **ì°¸ê³ **: `nvidia-device-plugin-daemonset`ì€ GPU ì‚¬ìš©ì„ ìœ„í•´ ê³„ì† ì‹¤í–‰ë˜ì–´ì•¼ í•˜ë¯€ë¡œ ì‚­ì œí•˜ì§€ ë§ˆì„¸ìš”.

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. ë…¸ë“œê°€ NotReady ìƒíƒœì¸ ê²½ìš°

containerd ì„¤ì • í…œí”Œë¦¿ì´ ì˜ëª»ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í…œí”Œë¦¿ì„ ì‚­ì œí•˜ê³  K3së¥¼ ì¬ì‹œì‘í•˜ì„¸ìš”:

```bash
sudo rm /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl
sudo systemctl restart k3s
```

ê·¸ í›„ 3ë‹¨ê³„ë¶€í„° ë‹¤ì‹œ ì§„í–‰í•˜ì„¸ìš”.

### 2. Device Pluginì—ì„œ "No devices found" ì—ëŸ¬

Pod ë‚´ì—ì„œ nvidia-smiê°€ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸:

```bash
kubectl exec -n kube-system <nvidia-device-plugin-pod-name> -- nvidia-smi
```

- ì‘ë™í•˜ë©´: `runtimeClassName: nvidia`ê°€ ì œëŒ€ë¡œ ì ìš©ëœ ê²ƒì…ë‹ˆë‹¤.
- ì‘ë™í•˜ì§€ ì•Šìœ¼ë©´: containerd ì„¤ì •ì—ì„œ nvidia runtimeì´ ê¸°ë³¸ìœ¼ë¡œ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

### 3. gpu-test Podì—ì„œ GPUê°€ ë³´ì´ì§€ ì•ŠëŠ” ê²½ìš°

Pod ì •ì˜ì— `runtimeClassName: nvidia`ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. K3sì—ì„œëŠ” ì´ ì„¤ì •ì´ í•„ìˆ˜ì…ë‹ˆë‹¤.

### 4. vLLM ë“±ì—ì„œ `No CUDA GPUs are available` ì—ëŸ¬

`nvidia-smi`ëŠ” ì‘ë™í•˜ëŠ”ë° PyTorchì—ì„œ `torch.cuda.is_available()`ì´ `False`ë¥¼ ë°˜í™˜í•˜ëŠ” ê²½ìš°ì…ë‹ˆë‹¤.

**ì›ì¸ 1: CDI ìŠ¤í™ ë¯¸ìƒì„±**

1ë‹¨ê³„ì—ì„œ `nvidia-ctk cdi generate` ëª…ë ¹ì„ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

```bash
# CDI ìŠ¤í™ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
ls /etc/cdi/nvidia.yaml

# ì—†ë‹¤ë©´ ìƒì„±
sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
sudo systemctl restart k3s
```

**ì›ì¸ 2: CUDA compat ë“œë¼ì´ë²„ ì¶©ëŒ (WSL2)**

vLLM ì´ë¯¸ì§€ ë‚´ë¶€ì˜ `/usr/local/cuda-xx.x/compat/libcuda.so.1`ì´ WSL2 í˜¸ìŠ¤íŠ¸ì˜ ì‹¤ì œ ë“œë¼ì´ë²„ë³´ë‹¤ ë¨¼ì € ë¡œë”©ë˜ì–´ CUDA ì´ˆê¸°í™”ê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ ê²½ìš° Podì˜ `env`ì— ë‹¤ìŒì„ ì¶”ê°€í•˜ì—¬ WSL2 ë“œë¼ì´ë²„ ê²½ë¡œë¥¼ ìš°ì„ ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.

```yaml
env:
  - name: LD_LIBRARY_PATH
    value: "/usr/lib/wsl/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64"
```

> ìì„¸í•œ ë‚´ìš©ì€ [vLLM ì„œë¹™ ê°€ì´ë“œ](/blog/k3s-vllm-serving)ì˜ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì„¹ì…˜ì„ ì°¸ê³ í•˜ì„¸ìš”.

---

## ğŸ“‹ ê²€ì¦ í™˜ê²½

- **OS**: Windows 11 + WSL2 Ubuntu 24.04
- **K3s**: v1.34.3+k3s1
- **NVIDIA Driver**: 591.44 (Windows) / 590.44.01 (WSL2)
- **NVIDIA Container Toolkit**: 1.18.2
- **GPU**: NVIDIA GeForce RTX 5060 Ti
- **ê²€ì¦ì¼**: 2026-02-08