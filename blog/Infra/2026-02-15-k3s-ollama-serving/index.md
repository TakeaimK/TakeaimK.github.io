---
slug: k3s-ollama-serving
title: "[Infra] K3sμ— Ollamaλ΅ LLM μ„λΉ™ν•κΈ°"
authors: [me]
tags: [k3s, ollama, qwen, llm, serving, gpu, nvidia]
---

μ§€λ‚ ν¬μ¤ν…μ—μ„ **vLLM**μ„ ν™μ©ν•΄ κ³ μ„±λ¥ LLM μ¶”λ΅  ν™κ²½μ„ κµ¬μ¶•ν–μµλ‹λ‹¤.
μ΄μ  μ‹λ¦¬μ¦μ λ§μ§€λ§‰, **2νƒ„**μ…λ‹λ‹¤! μ΄λ²μ—λ” κ°€λ³κ³  κ°„νΈν• **Ollama**λ¥Ό μ‚¬μ©ν•μ—¬ λ¨λΈμ„ λ°°ν¬ν•΄λ³΄κ² μµλ‹λ‹¤.

1νƒ„: μ••λ„μ μΈ μ„±λ¥κ³Ό ν™•μ¥μ„±, vLLM νΈ
**2νƒ„: κ°€λ³κ³  κ°„νΈν• λ΅μ»¬ μ‹¤ν–‰, Ollama νΈ**

μ΄λ² κΈ€μ—μ„λ” λ΅μ»¬ LLM μ‹¤ν–‰μ λ€λ…μ‚¬ **Ollama**λ¥Ό μ‚¬μ©ν•μ—¬, λ™μΌν• **Qwen3-4B** λ¨λΈμ„ K3s μ„μ—μ„ μ„λΉ™ν•κ³ , **Modelfile**λ΅ λ‚λ§μ μ»¤μ¤ν…€ λ¨λΈκΉμ§€ λ§λ“¤μ–΄λ΄…λ‹λ‹¤.

<!--truncate-->

## 1. μ™ OllamaμΈκ°€? (Why Ollama?)

μ§€λ‚ ν¬μ¤ν…μ—μ„ vLLMμ κ°•λ ¥ν•¨μ„ ν™•μΈν–μ§€λ§, μ†”μ§ν μ„¤μ •μ΄ κ½¤ λ³µμ΅ν–μ£ ? `LD_LIBRARY_PATH`, `gpu-memory-utilization`, `max-model-len`... μµμ…μ΄ ν•κ°€λ“μ΄μ—μµλ‹λ‹¤.

**Ollama**λ” μ΄λ° λ³µμ΅ν•¨μ„ λ¨λ‘ κ°μ¶”κ³ , **"λ¨λΈ μ‹¤ν–‰μ€ μ›λ μ΄λ ‡κ² μ‰¬μ›μ•Ό ν•λ‹¤"**λ” μ² ν•™μΌλ΅ λ§λ“¤μ–΄μ§„ λ„κµ¬μ…λ‹λ‹¤.

### 1.1 μ›μ»¤λ§¨λ“ μ‹¤ν–‰ (One-Command Simplicity)

Ollamaμ κ°€μ¥ ν° λ§¤λ ¥μ€ **ν• μ¤„μ΄λ©΄ λ**μ΄λΌλ” κ²ƒμ…λ‹λ‹¤.

```bash
# μ΄κ²ƒλ§μΌλ΅ λ¨λΈ λ‹¤μ΄λ΅λ“ + μ„λ²„ μ‹¤ν–‰ + λ€ν™” μ‹μ‘!
ollama run qwen3:4b
```

λ³µμ΅ν• μ„¤μ • νμΌλ„, ν† ν¬λ‚μ΄μ € μ΄κΈ°ν™”λ„, λ©”λ¨λ¦¬ κ³„μ‚°λ„ ν•„μ” μ—†μµλ‹λ‹¤. Ollamaκ°€ μ•μ•„μ„ ν•΄μ¤λ‹λ‹¤.

### 1.2 μμ²΄ λ¨λΈ λ μ§€μ¤νΈλ¦¬

vLLMμ€ HuggingFaceμ—μ„ λ¨λΈμ„ λ‹¤μ΄λ΅λ“ν•κ±°λ‚ λ΅μ»¬ κ²½λ΅λ¥Ό μ§μ ‘ λ§μ΄νΈν•΄μ•Ό ν–μµλ‹λ‹¤.
Ollamaλ” μμ²΄μ μΌλ΅ **λ¨λΈ λ μ§€μ¤νΈλ¦¬**λ¥Ό μ΄μν•©λ‹λ‹¤. Docker Hubμ²λΌ `pull`/`push` λ…λ Ήμ–΄λ΅ λ¨λΈμ„ κ΄€λ¦¬ν•  μ μμµλ‹λ‹¤.

```bash
# λ¨λΈ λ‹¤μ΄λ΅λ“ (Docker μ΄λ―Έμ§€ pullκ³Ό λ™μΌν• UX)
ollama pull qwen3:4b

# λ‹¤μ΄λ΅λ“λ λ¨λΈ λ©λ΅ ν™•μΈ
ollama list
```

### 1.3 Modelfile: λ‚λ§μ λ¨λΈ λ§λ“¤κΈ°

Ollamaλ§μ ν‚¬λ¬ ν”Όμ²μ…λ‹λ‹¤. **Dockerfile**μ²λΌ ν…μ¤νΈ νμΌ ν•λ‚λ΅ λ¨λΈμ μ‹μ¤ν… ν”„λ΅¬ν”„νΈ, νλΌλ―Έν„°, νλ¥΄μ†λ‚ λ“±μ„ μ •μν•μ—¬ **μ»¤μ¤ν…€ λ¨λΈ**μ„ λ§λ“¤ μ μμµλ‹λ‹¤.

```
FROM qwen3:4b
SYSTEM "λ‹Ήμ‹ μ€ ν•κµ­μ–΄ μ „λ¬Έ AI λΉ„μ„μ…λ‹λ‹¤. ν•­μƒ μ΅΄λ“λ§μ„ μ‚¬μ©ν•©λ‹λ‹¤."
PARAMETER temperature 0.7
PARAMETER num_ctx 8192
```

μ΄λ² κΈ€μ λ³΄λ„μ¤ μ„Ήμ…μ—μ„ μ§μ ‘ λ§λ“¤μ–΄λ³Ό μμ •μ΄λ‹, κΈ°λ€ν•΄ μ£Όμ„Έμ”!

### 1.4 vLLM vs Ollama: μ–Έμ  λ¬΄μ—‡μ„ μ“ΈκΉ?

| λΉ„κµ ν•­λ© | vLLM | Ollama |
| :--- | :--- | :--- |
| **μ£Όμ” λ©μ ** | ν”„λ΅λ•μ… μ„λΉ™ | λ΅μ»¬ κ°λ°/ν”„λ΅ν† νƒ€μ΄ν•‘ |
| **μ„¤μ • λ‚μ΄λ„** | λ†’μ (μµμ… λ‹¤μ) | λ§¤μ° λ‚®μ (μ›μ»¤λ§¨λ“) |
| **λ™μ‹ μ²λ¦¬ μ„±λ¥** | λ§¤μ° λ†’μ (PagedAttention) | λ³΄ν†µ |
| **λ¨λΈ ν¬λ§·** | HuggingFace (Safetensors) | GGUF (μ–‘μν™” νΉν™”) |
| **μ»¤μ¤ν„°λ§μ΄μ§•** | μ½”λ“ λ λ²¨ | Modelfile (μ„ μ–Έμ ) |
| **OpenAI API νΈν™** | β… μ™„μ „ νΈν™ | β… νΈν™ (μ‹¤ν—μ ) |
| **μ ν•©ν• ν™κ²½** | ν€/μ„λΉ„μ¤ μ΄μ | κ°μΈ κ°λ°/ν•™μµ |

> **ν• μ¤„ μ”μ•½**: λ™μ‹ μ ‘μ†μκ°€ λ§μ€ **ν”„λ΅λ•μ… μ„λΉ„μ¤**λΌλ©΄ vLLM, **λΉ λ¥΄κ² λ¨λΈμ„ ν…μ¤νΈ**ν•κ³  ν”„λ΅ν† νƒ€μ΄ν•‘ν•κ³  μ‹¶λ‹¤λ©΄ Ollama!

---

## 2. Ollama μ£Όμ” λ…λ Ήμ–΄ (Quick Reference)

Ollamaλ” CLI κΈ°λ°μΌλ΅ λ™μ‘ν•©λ‹λ‹¤. μ‹¤λ¬΄μ—μ„ μμ£Ό μ‚¬μ©ν•λ” ν•µμ‹¬ λ…λ Ήμ–΄λ¥Ό μ •λ¦¬ν•©λ‹λ‹¤.

| λ…λ Ήμ–΄ | μ„¤λ… | μμ‹ |
| :--- | :--- | :--- |
| `ollama pull` | λ¨λΈμ„ λ μ§€μ¤νΈλ¦¬μ—μ„ λ‹¤μ΄λ΅λ“ | `ollama pull qwen3:4b` |
| `ollama run` | λ¨λΈ μ‹¤ν–‰ (λ‹¤μ΄λ΅λ“ + λ€ν™”) | `ollama run qwen3:4b` |
| `ollama list` | λ‹¤μ΄λ΅λ“λ λ¨λΈ λ©λ΅ ν™•μΈ | `ollama list` |
| `ollama show` | λ¨λΈ μƒμ„Έ μ •λ³΄ ν™•μΈ | `ollama show qwen3:4b` |
| `ollama rm` | λ¨λΈ μ‚­μ  | `ollama rm qwen3:4b` |
| `ollama create` | Modelfileλ΅ μ»¤μ¤ν…€ λ¨λΈ μƒμ„± | `ollama create my-bot -f Modelfile` |
| `ollama ps` | ν„μ¬ λ΅λ“λ λ¨λΈ ν™•μΈ | `ollama ps` |

<details>
<summary>π”½ <strong>(ν΄λ¦­) Ollama μ£Όμ” ν™κ²½λ³€μ μ •λ¦¬</strong></summary>

Ollamaλ” ν™κ²½λ³€μλ¥Ό ν†µν•΄ μ„λ²„ λ™μ‘μ„ μ μ–΄ν•©λ‹λ‹¤. K3s λ°°ν¬ μ‹ Podμ `env`μ— μ„¤μ •ν•  μ μμµλ‹λ‹¤.

| ν™κ²½λ³€μ | κΈ°λ³Έκ°’ | μ„¤λ… |
| :--- | :--- | :--- |
| `OLLAMA_HOST` | `127.0.0.1:11434` | Ollama μ„λ²„ λ°”μΈλ”© μ£Όμ†. μ™Έλ¶€ μ ‘κ·Ό ν—μ© μ‹ `0.0.0.0:11434`λ΅ μ„¤μ • |
| `OLLAMA_MODELS` | `~/.ollama/models` | λ¨λΈ μ €μ¥ λ””λ ‰ν† λ¦¬ κ²½λ΅ |
| `OLLAMA_KEEP_ALIVE` | `5m` | λ¨λΈμ„ λ©”λ¨λ¦¬μ— μ μ§€ν•λ” μ‹κ°„. `0`μ΄λ©΄ μ¦‰μ‹ μ–Έλ΅λ“ |
| `OLLAMA_NUM_PARALLEL` | `1` | λ™μ‹ μ²λ¦¬ μ”μ²­ μ |
| `OLLAMA_MAX_LOADED_MODELS` | `1` | λ™μ‹μ— λ©”λ¨λ¦¬μ— λ΅λ“ν•  λ¨λΈ μ |
| `OLLAMA_DEBUG` | `false` | λ””λ²„κ·Έ λ΅κΉ… ν™μ„±ν™” |
| `OLLAMA_FLASH_ATTENTION` | `true` | Flash Attention μ‚¬μ© μ—¬λ¶€ |

> **μ°Έκ³ **: ν™κ²½λ³€μ λ©λ΅μ€ [κ³µμ‹ FAQ](https://github.com/ollama/ollama/blob/main/docs/faq.md)λ¥Ό μ°Έκ³ ν•μ„Έμ”.

</details>

---

## 3. μ‹¤μµ: Qwen3-4B μ„λΉ™ν•κΈ°

μ΄μ  K3s ν΄λ¬μ¤ν„°μ— Ollamaλ¥Ό λ°°ν¬ν•΄ λ΄…μ‹λ‹¤.
vLLM ν¬μ¤ν…κ³Ό λ™μΌν• **Qwen3-4B** λ¨λΈμ„ μ‚¬μ©ν•μ—¬, λ‘ λ„κµ¬μ μ°¨μ΄λ¥Ό μ§μ ‘ λκ»΄λ³΄μ„Έμ”.

### 3.1 Deployment Manifest μ‘μ„±

`ollama-qwen3.yaml` νμΌμ„ μ‘μ„±ν•©λ‹λ‹¤.
μ§€λ‚ μ‹κ°„μ— λ§λ“  `llm-serving` λ„¤μ„μ¤νμ΄μ¤μ— λ°°ν¬ν•κ² μµλ‹λ‹¤.

<details>
<summary>π“„ <strong>(ν΄λ¦­) ollama-qwen3.yaml νμΌ μ „μ²΄ λ‚΄μ© λ³΄κΈ°</strong></summary>

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-qwen3
  namespace: llm-serving
  labels:
    app: ollama-qwen3
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama-qwen3
  template:
    metadata:
      labels:
        app: ollama-qwen3
    spec:
      # [K3s ν•µμ‹¬] NVIDIA μ»¨ν…μ΄λ„ λ°νƒ€μ„ μ‚¬μ© λ…μ‹
      runtimeClassName: nvidia
      # κ³µμ  λ©”λ¨λ¦¬(SHM) μ ‘κ·Ό ν—μ© (GPU κ°„ ν†µμ‹  μ‹ ν•„μ”)
      hostIPC: true
      containers:
        - name: ollama
          # Ollama κ³µμ‹ Docker μ΄λ―Έμ§€ (μµμ‹  λ²„μ „)
          image: ollama/ollama:latest
          securityContext:
            privileged: true
          resources:
            limits:
              # GPU 1κ° ν• λ‹Ή
              nvidia.com/gpu: 1
              memory: "16Gi"
            requests:
              nvidia.com/gpu: 1
              memory: "8Gi"
          env:
            # Ollamaκ°€ λ¨λ“  μΈν„°νμ΄μ¤μ—μ„ μμ‹ ν•λ„λ΅ μ„¤μ •
            - name: OLLAMA_HOST
              value: "0.0.0.0:11434"
            # λ¨λΈ μ €μ¥ κ²½λ΅ (λ³Όλ¥¨ λ§μ΄νΈ κ²½λ΅μ™€ μΌμΉ)
            - name: OLLAMA_MODELS
              value: "/root/.ollama/models"
            # λ΅κ·Έ λ λ²¨ μ„¤μ • (λ””λ²„κΉ… μ‹ DEBUGλ΅ λ³€κ²½)
            - name: OLLAMA_DEBUG
              value: "false"
            # [WSL2 ν•„μ] νΈμ¤νΈ λ“λΌμ΄λ²„κ°€ λ¨Όμ € λ΅λ”©λλ„λ΅ κ²½λ΅ μ°μ„  λ°°μΉ
            - name: LD_LIBRARY_PATH
              value: "/usr/lib/wsl/lib:/usr/lib/wsl/drivers/nvmdsi.inf_amd64_83eb34a6b09136c0:/usr/local/nvidia/lib64:/usr/local/cuda/lib64"
            # NCCL κ΄€λ ¨ μ„¤μ • (WSL2 ν™κ²½ νΈν™μ„±)
            - name: NCCL_P2P_DISABLE
              value: "1"
            - name: NCCL_CUMEM_HOST_ENABLE
              value: "0"
            - name: NCCL_NVLS_ENABLE
              value: "0"
          ports:
            - containerPort: 11434
              name: http
          # Ollama μ„λ²„ ν—¬μ¤μ²΄ν¬ (λ£¨νΈ μ—”λ“ν¬μΈνΈ μ‚¬μ©)
          livenessProbe:
            httpGet:
              path: /
              port: 11434
            initialDelaySeconds: 30
            periodSeconds: 10
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /
              port: 11434
            initialDelaySeconds: 15
            periodSeconds: 5
            failureThreshold: 3
          startupProbe:
            httpGet:
              path: /
              port: 11434
            initialDelaySeconds: 10
            periodSeconds: 5
            failureThreshold: 30
          volumeMounts:
            # Ollama λ¨λΈ μ €μ¥μ† (νΈμ¤νΈμ™€ κ³µμ ν•μ—¬ μ¬λ‹¤μ΄λ΅λ“ λ°©μ§€)
            - name: ollama-data
              mountPath: /root/.ollama
      volumes:
        - name: ollama-data
          hostPath:
            # νΈμ¤νΈ(WSL2)μ Ollama λ°μ΄ν„° λ””λ ‰ν† λ¦¬
            path: /root/.ollama
            type: DirectoryOrCreate
---
apiVersion: v1
kind: Service
metadata:
  name: ollama-qwen3-service
  namespace: llm-serving
spec:
  selector:
    app: ollama-qwen3
  ports:
    - protocol: TCP
      port: 80
      targetPort: 11434
  type: ClusterIP
```

</details>

#### π“„ YAML νμΌ μƒμ„Έ μ„¤λ… (μ΄λ³΄μ ν•„λ…!)

vLLM λ°°ν¬μ™€ λΉ„κµν•λ©΄μ„ ν•µμ‹¬μ μΈ μ°¨μ΄μ μ„ μ‚΄ν΄λ΄…μ‹λ‹¤.

*   `image: ollama/ollama:latest`: Ollama κ³µμ‹ Docker μ΄λ―Έμ§€μ…λ‹λ‹¤. vLLMκ³Ό λ‹¬λ¦¬ λ³„λ„μ νƒκ·Έ(λ²„μ „) μ§€μ • μ—†μ΄λ„ GPUλ¥Ό μλ™μΌλ΅ κ°μ§€ν•©λ‹λ‹¤.
*   `containerPort: 11434`: Ollamaμ κΈ°λ³Έ ν¬νΈμ…λ‹λ‹¤. (vLLMμ€ 8000λ²)
*   `OLLAMA_HOST: "0.0.0.0:11434"`: **μ¤‘μ”!** κΈ°λ³Έκ°’μ΄ `127.0.0.1`μ΄λΌ Pod μ™Έλ¶€μ—μ„ μ ‘κ·Όν•  μ μ—†μµλ‹λ‹¤. λ°λ“μ‹ `0.0.0.0`μΌλ΅ λ°”κΏ”μ•Ό Serviceλ¥Ό ν†µν•΄ μ ‘κ·Όν•  μ μμµλ‹λ‹¤.
*   `OLLAMA_MODELS`: λ¨λΈμ΄ μ €μ¥λλ” κ²½λ΅μ…λ‹λ‹¤. `hostPath` λ³Όλ¥¨κ³Ό λ§¤μΉ­ν•μ—¬ νλ“λ¥Ό μ¬μ‹μ‘ν•΄λ„ λ¨λΈμ„ λ‹¤μ‹ λ‹¤μ΄λ΅λ“ν•μ§€ μ•μµλ‹λ‹¤.
*   `args`κ°€ **μ—†λ‹¤**λ” μ μ— μ£Όλ©ν•μ„Έμ”! vLLMκ³Ό λ‹¬λ¦¬ Ollamaλ” μ„λ²„λ§ λ„μ°λ©΄ λ©λ‹λ‹¤. λ¨λΈμ€ λ‚μ¤‘μ— `pull` λ…λ ΉμΌλ΅ λ‹¤μ΄λ΅λ“ν•©λ‹λ‹¤.
*   `LD_LIBRARY_PATH`: **(WSL2 ν•„μ)** vLLMκ³Ό λ™μΌν•κ² WSL2 νΈμ¤νΈ λ“λΌμ΄λ²„κ°€ λ¨Όμ € λ΅λ”©λλ„λ΅ κ²½λ΅λ¥Ό μ„¤μ •ν•©λ‹λ‹¤.
*   `livenessProbe`μ `path: /`: Ollamaλ” λ£¨νΈ κ²½λ΅(`/`)μ— μ ‘κ·Όν•λ©΄ `"Ollama is running"` λ©”μ‹μ§€λ¥Ό λ°ν™ν•©λ‹λ‹¤. vLLMμ `/health`μ™€ λ‹¤λ¦…λ‹λ‹¤.

> **vLLMκ³Όμ ν•µμ‹¬ μ°¨μ΄**: vLLMμ€ Deployment μ‹μ μ— `--model` μµμ…μΌλ΅ λ¨λΈκΉμ§€ μ§€μ •ν•κ³  ν•λ²μ— λ΅λ”©ν•©λ‹λ‹¤.
> Ollamaλ” μ„λ²„λ§ λ¨Όμ € λ„μ°κ³ , λ¨λΈμ€ μ΄ν›„μ— `ollama pull`λ΅ λ‹¤μ΄λ΅λ“ν•©λ‹λ‹¤. λ§μΉ Docker λ°λ¬μ„ λ¨Όμ € λ„μ°κ³ , μ΄λ―Έμ§€λ¥Ό λ‚μ¤‘μ— pull ν•λ” κ²ƒκ³Ό κ°™μ€ κ°λ…μ…λ‹λ‹¤.

### 3.2 λ°°ν¬ λ° λ¨λΈ λ‹¤μ΄λ΅λ“

μ‘μ„±ν• YAML νμΌμ„ K3s ν΄λ¬μ¤ν„°μ— μ μ©ν•©λ‹λ‹¤.

```bash
# λ„¤μ„μ¤νμ΄μ¤κ°€ μ—†λ‹¤λ©΄ λ¨Όμ € μƒμ„±
kubectl create namespace llm-serving

# λ°°ν¬ μ μ©
kubectl apply -f ollama-qwen3.yaml
```

#### π” μ§„ν–‰ μƒν™© λ¨λ‹ν„°λ§

```bash
# νλ“ μƒνƒλ¥Ό μ‹¤μ‹κ°„μΌλ΅ ν™•μΈ
kubectl get pods -n llm-serving -w
```

νλ“κ°€ `Running` μƒνƒκ°€ λλ©΄, Ollama μ„λ²„κ°€ μ¤€λΉ„λ κ²ƒμ…λ‹λ‹¤.
μ΄μ  λ¨λΈμ„ λ‹¤μ΄λ΅λ“ν•©λ‹λ‹¤.

```bash
# Pod λ‚΄λ¶€μ—μ„ λ¨λΈ λ‹¤μ΄λ΅λ“ μ‹¤ν–‰
kubectl exec -n llm-serving deploy/ollama-qwen3 -- ollama pull qwen3:4b
```

**[μ‹¤ν–‰ κ²°κ³Ό μμ‹]**

```
pulling manifest
pulling 3e4cb1417446: 100% β–•β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β– 2.5 GB
pulling 2d54db2b9bb2: 100% β–•β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β– 1.5 KB
pulling d18a5cc71b84: 100% β–•β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–  11 KB
pulling cff3f395ef37: 100% β–•β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–  120 B
pulling e18a783aae55: 100% β–•β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–β–  487 B
verifying sha256 digest
writing manifest
success
```

λ‹¤μ΄λ΅λ“κ°€ μ™„λ£λλ©΄, λ¨λΈμ΄ μ •μƒμ μΌλ΅ λ“±λ΅λμ—λ”μ§€ ν™•μΈν•©λ‹λ‹¤.

```bash
kubectl exec -n llm-serving deploy/ollama-qwen3 -- ollama list
```

```
NAME        ID              SIZE      MODIFIED
qwen3:4b    359d7dd4bcda    2.5 GB    21 seconds ago
```

> **π’΅ Tip**: `hostPath`λ΅ `/root/.ollama`λ¥Ό λ§μ΄νΈν–κΈ° λ•λ¬Έμ—, νλ“λ¥Ό μ‚­μ ν•κ³  λ‹¤μ‹ λ§λ“¤μ–΄λ„ λ¨λΈμ΄ μ μ§€λ©λ‹λ‹¤. λ‹¤μ‹ λ‹¤μ΄λ΅λ“ν•  ν•„μ”κ°€ μ—†μµλ‹λ‹¤!

#### π§ νΈλ¬λΈ”μν…: GPU κ΄€λ ¨ λ¬Έμ 

vLLM νΈκ³Ό λ™μΌν•κ², GPU μΈμ‹ μ‹¤ν¨ μ‹ λ‹¤μμ„ ν™•μΈν•μ„Έμ”:

> **μ¦μƒ: GPUλ¥Ό μΈμ‹ν•μ§€ λ»ν•λ” κ²½μ°**
> 1. **CDI μ¤ν™ λ―Έμƒμ„±**: [K3s μ„¤μΉ κ°€μ΄λ“](../2026-02-07-k3s-wsl-install/index.md)μ 1λ‹¨κ³„μ—μ„ `sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml` λ…λ Ήμ„ μ‹¤ν–‰ν–λ”μ§€ ν™•μΈν•μ„Έμ”.
> 2. **CUDA compat λ“λΌμ΄λ²„ μ¶©λ**: YAMLμ `env`μ— `LD_LIBRARY_PATH` μ„¤μ •μ΄ ν¬ν•¨λμ–΄ μλ”μ§€ ν™•μΈν•μ„Έμ”. (WSL2 ν™κ²½ ν•„μ)

---

## 4. API ν…μ¤νΈ: λ„¤μ΄ν‹°λΈ API & OpenAI νΈν™ API

Ollamaλ” **λ‘ κ°€μ§€ API**λ¥Ό μ κ³µν•©λ‹λ‹¤.
1. **λ„¤μ΄ν‹°λΈ API** (`/api/chat`, `/api/generate`): Ollama κ³ μ μ API
2. **OpenAI νΈν™ API** (`/v1/chat/completions`): κΈ°μ΅΄ OpenAI ν΄λΌμ΄μ–ΈνΈλ¥Ό κ·Έλ€λ΅ μ‚¬μ© κ°€λ¥

### 4.1 ν¬νΈ ν¬μ›λ”© μ„¤μ •

```bash
# λ΅μ»¬ ν¬νΈ 11434λ²μ„ μ„λΉ„μ¤ ν¬νΈ 80λ²μ— μ—°κ²°
kubectl port-forward svc/ollama-qwen3-service 11434:80 -n llm-serving
```

μ΄μ  ν„°λ―Έλ„μ„ ν•λ‚ λ” μ—΄μ–΄μ„ ν…μ¤νΈλ¥Ό μ§„ν–‰ν•©λ‹λ‹¤.

### 4.2 λ„¤μ΄ν‹°λΈ API ν…μ¤νΈ

Ollamaμ κ³ μ  APIμΈ `/api/chat`μ„ μ‚¬μ©ν•΄ λ΄…λ‹λ‹¤.

```bash
# λ¨λΈ λ©λ΅ ν™•μΈ
curl -s http://localhost:11434/api/tags | python3 -m json.tool
```

```json
{
    "models": [
        {
            "name": "qwen3:4b",
            "model": "qwen3:4b",
            "size": 2497293931,
            "details": {
                "format": "gguf",
                "family": "qwen3",
                "parameter_size": "4.0B",
                "quantization_level": "Q4_K_M"
            }
        }
    ]
}
```

μ±„ν… APIλ¥Ό νΈμ¶ν•©λ‹λ‹¤. `stream: false`λ΅ μ„¤μ •ν•λ©΄ μ‘λ‹µμ„ ν• λ²μ— λ°›μ„ μ μμµλ‹λ‹¤.

```bash
curl -s http://localhost:11434/api/chat \
  -d '{
    "model": "qwen3:4b",
    "messages": [
      {"role": "user", "content": "3.11κ³Ό 3.9 μ¤‘ μ–΄λ μ«μκ°€ λ” ν°κ°€μ”?"}
    ],
    "stream": false
  }' | python3 -m json.tool
```

**[μ‹¤ν–‰ κ²°κ³Ό μμ‹]**

```json
{
    "model": "qwen3:4b",
    "message": {
        "role": "assistant",
        "content": "3.9κ°€ λ” ν° μ«μμ…λ‹λ‹¤.\n(3.9λ” 3.90μ΄λ―€λ΅, 3.11λ³΄λ‹¤ 0.79κ°€ λ” ν½λ‹λ‹¤.)",
        "thinking": "Okay, so I need to figure out whether 3.11 or 3.9 is the larger number..."
    },
    "done": true,
    "total_duration": 49690154045,
    "eval_count": 356,
    "eval_duration": 3247069741
}
```

> **μ£Όλ©!** μ‘λ‹µμ— `thinking` ν•„λ“κ°€ λ³„λ„λ΅ λ¶„λ¦¬λμ–΄ μμµλ‹λ‹¤. vLLMμ—μ„λ” `content` μ•μ— `<think>...</think>` νƒκ·Έκ°€ ν¬ν•¨λμ–΄ μμ—μ§€λ§, Ollamaλ” μ‚¬κ³  κ³Όμ •μ„ λ³„λ„ ν•„λ“λ΅ κΉ”λ”ν•κ² λ¶„λ¦¬ν•΄μ¤λ‹λ‹¤.

### 4.3 OpenAI νΈν™ API ν…μ¤νΈ

κΈ°μ΅΄ OpenAI ν΄λΌμ΄μ–ΈνΈμ™€ νΈν™λλ” `/v1/chat/completions` μ—”λ“ν¬μΈνΈλ„ μ§€μ›ν•©λ‹λ‹¤.
vLLM νΈμ ν…μ¤νΈ μ½”λ“μ™€ κ±°μ λ™μΌν•κ² μ‚¬μ©ν•  μ μμµλ‹λ‹¤!

```bash
python3 -c "import urllib.request, json; \
print(json.load(urllib.request.urlopen(urllib.request.Request( \
    'http://localhost:11434/v1/chat/completions', \
    data=json.dumps({ \
        'model': 'qwen3:4b', \
        'messages': [{'role': 'user', 'content': '3.11κ³Ό 3.9 μ¤‘ μ–΄λ μ«μκ°€ λ” ν°κ°€μ”?'}], \
        'temperature': 0.7 \
    }).encode('utf-8'), \
    headers={'Content-Type': 'application/json'} \
)))['choices'][0]['message']['content'])"
```

vLLMκ³Ό λ™μΌν• OpenAI ν¬λ§·μΌλ΅ μ‘λ‹µμ΄ μµλ‹λ‹¤!

> **π’΅ vLLM β†’ Ollama λ§μ΄κ·Έλ μ΄μ… ν**
> OpenAI νΈν™ APIλ¥Ό μ‚¬μ©ν•λ©΄, μ½”λ“ λ³€κ²½ μ—†μ΄ `base_url`λ§ λ°”κΎΈλ©΄ λ©λ‹λ‹¤.
> - vLLM: `http://localhost:8000/v1/chat/completions`
> - Ollama: `http://localhost:11434/v1/chat/completions`

---

## 5. λ³΄λ„μ¤: Ingressλ΅ ν¬νΈν¬μ›λ”© μ—†μ΄ μ ‘κ·Όν•κΈ°

vLLM νΈκ³Ό λ™μΌν•κ², **Ingress**λ¥Ό μ„¤μ •ν•μ—¬ ν¬νΈν¬μ›λ”© μ—†μ΄ APIμ— μ ‘κ·Όν•  μ μμµλ‹λ‹¤.

### 5.1 Ingress Manifest μ‘μ„±

`ollama-ingress.yaml` νμΌμ„ μ‘μ„±ν•©λ‹λ‹¤. Ollamaλ” λ„¤μ΄ν‹°λΈ API(`/api`)μ™€ OpenAI νΈν™ API(`/v1`) λ‘ κ°€μ§€ κ²½λ΅λ¥Ό λ¨λ‘ λΌμ°ν…ν•΄μ•Ό ν•©λ‹λ‹¤.

```yaml
# Ollama Ingress μ„¤μ •
# K3s λ‚΄μ¥ Traefik Ingress Controllerλ¥Ό μ‚¬μ©ν•μ—¬
# ν¬νΈ ν¬μ›λ”© μ—†μ΄ Ollama APIμ— μ ‘κ·Όν•  μ μλ„λ΅ κµ¬μ„±ν•©λ‹λ‹¤.
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ollama-ingress
  namespace: llm-serving
  annotations:
    # Traefikμ„ Ingress Controllerλ΅ μ‚¬μ©
    traefik.ingress.kubernetes.io/router.entrypoints: web
spec:
  # K3s λ‚΄μ¥ Traefik Ingress Class μ§€μ •
  ingressClassName: traefik
  rules:
    # DNSκ°€ μ—†μΌλ―€λ΅ localhostλ¥Ό νΈμ¤νΈλ΅ μ‚¬μ©
    - host: localhost
      http:
        paths:
          # Ollama λ„¤μ΄ν‹°λΈ API κ²½λ΅ (/api/chat, /api/generate λ“±)
          - path: /api
            pathType: Prefix
            backend:
              service:
                name: ollama-qwen3-service
                port:
                  number: 80
          # OpenAI νΈν™ API κ²½λ΅ (/v1/chat/completions λ“±)
          - path: /v1
            pathType: Prefix
            backend:
              service:
                name: ollama-qwen3-service
                port:
                  number: 80
```

#### π“„ YAML νμΌ μƒμ„Έ μ„¤λ…

vLLMμ Ingressμ™€ λΉ„κµν–μ„ λ• ν•µμ‹¬ μ°¨μ΄μ μ€ λ‹¤μκ³Ό κ°™μµλ‹λ‹¤:

*   `path: /api`: Ollama λ„¤μ΄ν‹°λΈ APIμ© κ²½λ΅μ…λ‹λ‹¤. vLLMμ—λ” μ—†λ κ²½λ΅μ…λ‹λ‹¤.
*   `path: /v1`: OpenAI νΈν™ API κ²½λ΅λ” vLLMκ³Ό λ™μΌν•©λ‹λ‹¤.
*   `/health` κ²½λ΅κ°€ **μ—†μµλ‹λ‹¤**: Ollamaλ” λ£¨νΈ κ²½λ΅(`/`)λ΅ ν—¬μ¤μ²΄ν¬ν•λ―€λ΅, λ³„λ„μ `/health` κ²½λ΅ λΌμ°ν…μ΄ ν•„μ” μ—†μµλ‹λ‹¤.

> β οΈ **μ£Όμ**: vLLM Ingressλ¥Ό λ™μ‹μ— μ‚¬μ©ν•κ³  μλ‹¤λ©΄, `/v1` κ²½λ΅κ°€ μ¶©λν•  μ μμµλ‹λ‹¤. ν•λ‚λ§ μ‚¬μ©ν•κ±°λ‚, `host`λ¥Ό λ‹¤λ¥΄κ² μ„¤μ •ν•μ„Έμ”.

### 5.2 Ingress λ°°ν¬ λ° ν™•μΈ

```bash
# Ingress λ°°ν¬
kubectl apply -f ollama-ingress.yaml

# μ „μ²΄ λ¦¬μ†μ¤ ν™•μΈ
kubectl get all,ingress -n llm-serving
```

**[μ‹¤ν–‰ κ²°κ³Ό μμ‹]**

```
NAME                                READY   STATUS    RESTARTS   AGE
pod/ollama-qwen3-85f9bf8c54-ldd47   1/1     Running   0          14m

NAME                           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
service/ollama-qwen3-service   ClusterIP   10.43.184.24   <none>        80/TCP    14m

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/ollama-qwen3   1/1     1            1           14m

NAME                                       CLASS     HOSTS       ADDRESS         PORTS   AGE
ingress.networking.k8s.io/ollama-ingress   traefik   localhost   172.22.239.53   80      6s
```

### 5.3 Ingressλ¥Ό ν†µν• API ν…μ¤νΈ

ν¬νΈν¬μ›λ”© μ—†μ΄ λ°”λ΅ APIλ¥Ό νΈμ¶ν•©λ‹λ‹¤!

```bash
# λ¨λΈ λ©λ΅ ν™•μΈ (Ingress κ²½μ )
curl -s -H 'Host: localhost' http://localhost/api/tags | python3 -m json.tool

# OpenAI νΈν™ API λ¨λΈ λ©λ΅ (Ingress κ²½μ )
curl -s -H 'Host: localhost' http://localhost/v1/models | python3 -m json.tool
```

> **π’΅ ν¬νΈν¬μ›λ”© vs Ingress λΉ„κµ**
>
> | κµ¬λ¶„ | ν¬νΈν¬μ›λ”© | Ingress |
> | :--- | :--- | :--- |
> | **URL** | `http://localhost:11434/api/...` | `http://localhost/api/...` |
> | **ν„°λ―Έλ„ μ μ ** | λ³„λ„ ν„°λ―Έλ„ ν•„μ” | λ¶ν•„μ” |
> | **μ•μ •μ„±** | ν„°λ―Έλ„ μΆ…λ£ μ‹ λκΉ€ | ν•­μƒ μ μ§€ |
> | **μ©λ„** | κ°λ°/λ””λ²„κΉ… | κ°λ°~μ΄μ μ „ λ‹¨κ³„ |

---

## 6. λ³΄λ„μ¤: Modelfileλ΅ μ»¤μ¤ν…€ λ¨λΈ λ§λ“¤κΈ°

Ollamaμ μ§„μ •ν• κ°•μ μΈ **Modelfile**μ„ ν™μ©ν•΄ λ‚λ§μ μ»¤μ¤ν…€ λ¨λΈμ„ λ§λ“¤μ–΄ λ΄…μ‹λ‹¤.

### 6.1 Modelfileμ΄λ€?

Modelfileμ€ Dockerμ Dockerfileκ³Ό λΉ„μ·ν• κ°λ…μ…λ‹λ‹¤. ν…μ¤νΈ νμΌ ν•λ‚λ΅ λ¨λΈμ ν–‰λ™μ„ μ •μν•©λ‹λ‹¤.

| λ…λ Ήμ–΄ | μ„¤λ… | ν•„μ |
| :--- | :--- | :--- |
| `FROM` | λ² μ΄μ¤ λ¨λΈ μ§€μ • | β… |
| `SYSTEM` | μ‹μ¤ν… ν”„λ΅¬ν”„νΈ (λ¨λΈμ μ—­ν• /μ„±κ²© μ„¤μ •) | |
| `PARAMETER` | λ¨λΈ νλΌλ―Έν„° μ„¤μ • (temperature λ“±) | |
| `TEMPLATE` | ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ μ»¤μ¤ν„°λ§μ΄μ§• | |
| `ADAPTER` | LoRA μ–΄λ‘ν„° μ μ© | |
| `MESSAGE` | λ€ν™” νμ¤ν† λ¦¬ μ‚¬μ „ μ‹λ”© | |
| `LICENSE` | λΌμ΄μ„ μ¤ μ •λ³΄ | |

### 6.2 ν•κµ­μ–΄ μ „λ¬Έ λΉ„μ„ λ§λ“¤κΈ°

μ‹¤μ λ΅ μ»¤μ¤ν…€ λ¨λΈμ„ λ§λ“¤μ–΄ λ΄…μ‹λ‹¤. Pod λ‚΄λ¶€μ—μ„ μ§μ ‘ μ‹¤ν–‰ν•©λ‹λ‹¤.

```bash
# 1. Modelfile μ‘μ„± (Pod λ‚΄λ¶€μ—μ„ μ‹¤ν–‰)
kubectl exec -n llm-serving deploy/ollama-qwen3 -- bash -c 'cat > /tmp/Modelfile << EOF
FROM qwen3:4b

# μ‹μ¤ν… ν”„λ΅¬ν”„νΈ: λ¨λΈμ νλ¥΄μ†λ‚ μ •μ
SYSTEM """λ‹Ήμ‹ μ€ "μ§€λ‹"λΌλ” μ΄λ¦„μ ν•κµ­μ–΄ AI λΉ„μ„μ…λ‹λ‹¤.
λ‹¤μ κ·μΉ™μ„ λ°λ“μ‹ λ”°λ¦…λ‹λ‹¤:
1. ν•­μƒ μ΅΄λ“λ§(ν•©μ‡Όμ²΄)μ„ μ‚¬μ©ν•©λ‹λ‹¤.
2. λ‹µλ³€μ€ κ°„κ²°ν•κ³  λ…ν™•ν•κ² ν•©λ‹λ‹¤.
3. κΈ°μ μ  λ‚΄μ©μ€ λΉ„μ λ¥Ό λ“¤μ–΄ μ‰½κ² μ„¤λ…ν•©λ‹λ‹¤.
4. Thinking κ³Όμ • μ—†μ΄ λ°”λ΅ λ‹µλ³€ν•©λ‹λ‹¤."""

# νλΌλ―Έν„° μ„¤μ •
PARAMETER temperature 0.7
PARAMETER num_ctx 8192
PARAMETER top_p 0.9

# Thinking λΉ„ν™μ„±ν™”
PARAMETER /no_think true
EOF'

# 2. μ»¤μ¤ν…€ λ¨λΈ μƒμ„±
kubectl exec -n llm-serving deploy/ollama-qwen3 -- ollama create jini-bot -f /tmp/Modelfile
```

**[μ‹¤ν–‰ κ²°κ³Ό μμ‹]**

```
gathering model components
using existing layer sha256:3e4cb1417446...
creating new layer sha256:a1b2c3d4e5f6...
writing manifest
success
```

### 6.3 μ»¤μ¤ν…€ λ¨λΈ ν…μ¤νΈ

λ§λ“¤μ–΄μ§„ `jini-bot` λ¨λΈμ„ ν…μ¤νΈν•΄ λ΄…μ‹λ‹¤.

```bash
# λ¨λΈ λ©λ΅ ν™•μΈ - jini-botμ΄ μ¶”κ°€λμ—λ”μ§€ ν™•μΈ
kubectl exec -n llm-serving deploy/ollama-qwen3 -- ollama list

# API νΈμ¶ ν…μ¤νΈ
curl -s http://localhost:11434/api/chat \
  -d '{
    "model": "jini-bot",
    "messages": [
      {"role": "user", "content": "μΏ λ²„λ„¤ν‹°μ¤κ°€ λ­”μ§€ μ‰½κ² μ„¤λ…ν•΄μ¤"}
    ],
    "stream": false
  }' | python3 -m json.tool
```

μ»¤μ¤ν…€ μ‹μ¤ν… ν”„λ΅¬ν”„νΈκ°€ μ μ©λμ–΄, "μ§€λ‹" νλ¥΄μ†λ‚λ΅ μ΅΄λ“λ§μ„ μ‚¬μ©ν•λ©° λΉ„μ λ¥Ό λ“¤μ–΄ μ„¤λ…ν•λ” λ¨μµμ„ ν™•μΈν•  μ μμµλ‹λ‹¤.

> **π’΅ ν™μ© μ•„μ΄λ””μ–΄**
> *   κ³ κ° μ‘λ€ λ΄‡: νμ‚¬μ FAQλ¥Ό μ‹μ¤ν… ν”„λ΅¬ν”„νΈμ— λ„£κ³  μ „μ© λ΄‡ μƒμ„±
> *   μ½”λ“ λ¦¬λ·°μ–΄: μ½”λ”© μ»¨λ²¤μ…μ„ μ •μν•κ³  μ½”λ“ λ¦¬λ·° μ „λ¬Έ λ¨λΈ μƒμ„±
> *   λ²μ—­κ°€: νΉμ • λ„λ©”μΈ μ©μ–΄μ§‘μ„ μ‹μ¤ν… ν”„λ΅¬ν”„νΈμ— ν¬ν•¨

---

## 7. λ§μΉλ©°: μ‹λ¦¬μ¦ μ΄μ •λ¦¬

2νμ— κ±Έμ³ K3s μ„μ—μ„ LLMμ„ μ„λΉ™ν•λ” λ‘ κ°€μ§€ λ°©λ²•μ„ λ‹¤λ¤μµλ‹λ‹¤.

| κµ¬λ¶„ | 1νƒ„: vLLM | 2νƒ„: Ollama |
| :--- | :--- | :--- |
| **μ„¤μ • νμΌ** | YAML λ‚΄ `args`λ΅ λ¨λΈ+μµμ… μ§€μ • | YAMLμ€ μ„λ²„λ§, λ¨λΈμ€ `pull`λ΅ λ³„λ„ κ΄€λ¦¬ |
| **λ¨λΈ ν¬λ§·** | HuggingFace (Safetensors) | GGUF (μ–‘μν™” νΉν™”) |
| **API** | OpenAI νΈν™ (`/v1/...`) | λ„¤μ΄ν‹°λΈ (`/api/...`) + OpenAI νΈν™ |
| **μ»¤μ¤ν„°λ§μ΄μ§•** | μ½”λ“ μμ¤€ μ„¤μ • | Modelfile (μ„ μ–Έμ ) |
| **κ°•μ ** | λ™μ‹ μ²λ¦¬, κ³ μ„±λ¥ | κ°„νΈν•¨, λΉ λ¥Έ ν”„λ΅ν† νƒ€μ΄ν•‘ |

λ‘ λ„κµ¬ λ¨λ‘ **κ°™μ€ K3s μΈν”„λΌ** μ„μ—μ„ λ™μ‘ν•λ©°, λ©μ μ— λ”°λΌ μ„ νƒν•λ©΄ λ©λ‹λ‹¤.
μ΄ μ‹λ¦¬μ¦λ¥Ό ν†µν•΄ μ—¬λ¬λ¶„μ λ΅μ»¬ ν™κ²½μ΄ **λ‚λ§μ AI μ„λ²„**λ΅ κ±°λ“­λ‚κΈΈ λ°”λλ‹λ‹¤! π€
